{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) # Adds the parent folder to sys.path\n",
    "from utils import *\n",
    "from Neural_Nets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert an integer to 4 binaries (bits)\n",
    "def int_to_four_bins(n):\n",
    "    return np.array(list(np.binary_repr(int(n), width=4))).astype(int)\n",
    "\n",
    "# This function computes the average accuracy per binary\n",
    "def compute_bitwise_accuracy(preds, targets):\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "# This function computes the strict accuracy: it counts how many entire binary vectors are predicted exactly right\n",
    "def compute_exact_match_accuracy(preds, targets):\n",
    "    return torch.all(preds == targets, dim=1).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNet(torch.nn.Module):\n",
    "    \"\"\"Simple class to implement a feed-forward neural network in PyTorch.\n",
    "    Attributes:\n",
    "        layers: list of torch.nn.Linear layers to be applied in forward pass.\n",
    "        activation: activation function to be applied between layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, shape, activation=None):\n",
    "        \"\"\"Constructor for FFNet.\n",
    "        Arguments:\n",
    "            shape: list of ints describing network shape, including input & output size.\n",
    "            activation: a torch.nn function specifying the network activation.\n",
    "        \"\"\"\n",
    "        super(FFNet, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.layers = []\n",
    "        self.activation = activation \n",
    "        for ii in range(0,len(shape)-1):\n",
    "            self.layers.append(torch.nn.Linear(shape[ii],shape[ii+1]))\n",
    "        self.layers = torch.nn.ModuleList(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Performs a forward pass on x, a numpy array of size (-1,shape[0])\"\n",
    "        for ii in range(len(self.layers)-1):\n",
    "            x = self.layers[ii](x)\n",
    "            if self.activation:\n",
    "                x = self.activation(x)\n",
    "\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "\n",
    "    def __init__(self, prob_features):\n",
    "        \"\"\"\n",
    "        Constructor for Regression class.\n",
    "        \"\"\"\n",
    "        self.prob_features = prob_features\n",
    "        self.n_bin = 4\n",
    "        self.num_train, self.num_test = 0, 0\n",
    "        self.model, self.model_fn = None, None\n",
    "\n",
    "    def construct_features(self, params):\n",
    "        prob_features = self.prob_features\n",
    "        feature_vec = np.array([])\n",
    "\n",
    "        for feature in prob_features:\n",
    "            if feature == \"x0\":\n",
    "                x0 = params['x0']\n",
    "                feature_vec = np.hstack((feature_vec, x0))\n",
    "            elif feature == \"xg\":\n",
    "                xg = params['xg'] \n",
    "                feature_vec = np.hstack((feature_vec, xg))\n",
    "            elif feature == \"obstacles\":\n",
    "                obstacles = params['obstacles']\n",
    "                feature_vec = np.hstack((feature_vec, np.reshape(obstacles, (4*self.n_obs))))\n",
    "            elif feature == \"obstacles_map\":\n",
    "                continue\n",
    "            else:\n",
    "                print('Feature {} is unknown'.format(feature))\n",
    "        return feature_vec\n",
    "\n",
    "    def setup_data(self, n_features, train_data):\n",
    "        \"\"\"\n",
    "        Reads in data and constructs strategy dictionary\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.X_train = train_data[0]\n",
    "        self.Y_train = train_data[3]\n",
    "        self.n_y = self.Y_train[0].size # will be the dimension of the output\n",
    "        self.y_shape = self.Y_train[0].shape\n",
    "        self.num_train = self.Y_train.shape[0]        \n",
    "\n",
    "        self.features = np.zeros((self.num_train, self.n_features))\n",
    "        self.labels = np.zeros((self.num_train, self.n_y))\n",
    "        self.outputs = np.zeros((self.num_train, self.n_y*self.n_bin))\n",
    "        \n",
    "        for ii in range(self.num_train):\n",
    "            self.labels[ii] = np.reshape(self.Y_train[ii,:,:], (self.n_y))\n",
    "            self.outputs[ii] = np.hstack([int_to_four_bins(val) for val in (self.labels[ii])])\n",
    "            prob_params = {}\n",
    "            for k in self.X_train:\n",
    "                prob_params[k] = self.X_train[k][ii]\n",
    "            self.features[ii] = self.construct_features(prob_params)\n",
    "\n",
    "    def setup_network(self, depth=3, neurons=32, device_id=0):\n",
    "        self.device = torch.device('cuda:{}'.format(device_id))\n",
    "        \n",
    "        ff_shape = [self.n_features]\n",
    "        for ii in range(depth):\n",
    "            ff_shape.append(neurons)\n",
    "        ff_shape.append(self.n_y*self.n_bin)\n",
    "\n",
    "        self.model = FFNet(ff_shape, activation=torch.nn.ReLU()).to(device=self.device)\n",
    "\n",
    "        # file names for PyTorch models\n",
    "        now = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "        model_fn = 'regression_{}.pt'\n",
    "        model_fn = os.path.join(os.getcwd(), model_fn)\n",
    "        self.model_fn = model_fn.format(now)\n",
    "\n",
    "    def load_network(self, fn_regressor_model):\n",
    "        if os.path.exists(fn_regressor_model):\n",
    "            print('Loading presaved regression model from {}'.format(fn_regressor_model))\n",
    "            self.model.load_state_dict(torch.load(fn_regressor_model))\n",
    "            self.model_fn = fn_regressor_model\n",
    "\n",
    "    def train(self, training_params, verbose=True):\n",
    "        # Unpack training params\n",
    "        BATCH_SIZE = training_params['BATCH_SIZE']\n",
    "        TEST_BATCH_SIZE = training_params['TEST_BATCH_SIZE']\n",
    "        TRAINING_EPOCHS = training_params['TRAINING_EPOCHS']\n",
    "        CHECKPOINT_AFTER = training_params['CHECKPOINT_AFTER']\n",
    "        SAVEPOINT_AFTER = training_params['SAVEPOINT_AFTER']\n",
    "        LEARNING_RATE = training_params['LEARNING_RATE']\n",
    "        WEIGHT_DECAY = training_params['WEIGHT_DECAY']\n",
    "        EARLY_STOPPING_PATIENCE = training_params['EARLY_STOPPING_PATIENCE']\n",
    "\n",
    "        model = self.model\n",
    "        device = self.device\n",
    "\n",
    "        # Prepare dataset\n",
    "        X_tensor = torch.from_numpy(self.features).float()\n",
    "        Y_tensor = torch.from_numpy(self.outputs).float()\n",
    "        full_dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "\n",
    "        # Split into train/val\n",
    "        num_total = len(full_dataset)\n",
    "        num_train = int(0.9 * num_total)\n",
    "        train_dataset = TensorDataset(X_tensor[:num_train], Y_tensor[:num_train])\n",
    "        val_dataset   = TensorDataset(X_tensor[num_train:], Y_tensor[num_train:])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        epochs_since_improvement = 0\n",
    "\n",
    "        itr = 1\n",
    "        for epoch in range(TRAINING_EPOCHS):\n",
    "            model.train()\n",
    "            for inputs, y_true in train_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                y_true = y_true.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, y_true)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if itr % CHECKPOINT_AFTER == 0:\n",
    "                    # Evaluate on validation set\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        val_loss_total = 0\n",
    "                        bitwise_accs = []\n",
    "                        exact_match_accs = []\n",
    "\n",
    "                        for val_inputs, val_targets in val_loader:\n",
    "                            val_inputs = val_inputs.to(device)\n",
    "                            val_targets = val_targets.to(device)\n",
    "\n",
    "                            val_logits = model(val_inputs)\n",
    "                            val_loss = loss_fn(val_logits, val_targets)\n",
    "                            val_loss_total += val_loss.item()\n",
    "\n",
    "                            val_probs = Sigmoid()(val_logits)\n",
    "                            val_preds = (val_probs > 0.5).int()\n",
    "                            val_targets_int = val_targets.int()\n",
    "\n",
    "                            bitwise_accs.append(compute_bitwise_accuracy(val_preds, val_targets_int))\n",
    "                            exact_match_accs.append(compute_exact_match_accuracy(val_preds, val_targets_int))\n",
    "\n",
    "                        avg_val_loss = val_loss_total / len(val_loader)\n",
    "                        avg_bitwise_acc = np.mean(bitwise_accs)\n",
    "                        # avg_exact_acc = np.mean(exact_match_accs)\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f\"[Iter {itr}] Validation loss: {avg_val_loss:.4f} | \"\n",
    "                                f\"Validation accuracy (bitwise): {avg_bitwise_acc:.4f}\")\n",
    "                            \n",
    "                        # Check for early stopping\n",
    "                        if avg_val_loss < best_val_loss - 1e-3:\n",
    "                            best_val_loss = avg_val_loss\n",
    "                            epochs_since_improvement = 0\n",
    "                        else:\n",
    "                            epochs_since_improvement += 1\n",
    "                            if epochs_since_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "                                print(f\"Early stopping: no improvement for {EARLY_STOPPING_PATIENCE} checks\")\n",
    "                                torch.save(model.state_dict(), self.model_fn)\n",
    "                                print(f\"Final model saved at {self.model_fn}\")\n",
    "                                return  # Exit training early\n",
    "\n",
    "                    model.train()\n",
    "\n",
    "                if itr % SAVEPOINT_AFTER == 0:\n",
    "                    torch.save(model.state_dict(), self.model_fn)\n",
    "                    if verbose:\n",
    "                        print(f\"[Iter {itr}] Saved model at {self.model_fn}\")\n",
    "\n",
    "                itr += 1\n",
    "\n",
    "        # Save final model\n",
    "        torch.save(model.state_dict(), self.model_fn)\n",
    "        print(f\"Final model saved at {self.model_fn}\")\n",
    "        print(\"Done training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = os.getcwd()\n",
    "relative_path = os.path.abspath(\"..\")\n",
    "dataset_fn = relative_path + '/data' + '/single.p'\n",
    "prob_features = ['x0', 'xg']\n",
    "\n",
    "data_file = open(dataset_fn,'rb')\n",
    "all_data = pickle.load(data_file)\n",
    "data_file.close()\n",
    "data_list = []\n",
    "num_train = len(all_data)\n",
    "\n",
    "X0 = np.vstack([all_data[ii]['x0'].T for ii in range(num_train)])  \n",
    "XG = np.vstack([all_data[ii]['xg'].T for ii in range(num_train)])  \n",
    "OBS = np.vstack([all_data[ii]['xg'].T for ii in range(num_train)])  \n",
    "YY = np.concatenate([all_data[ii]['YY'].astype(int) for ii in range(num_train)], axis=1).transpose(1,0,2)\n",
    "\n",
    "train_data = [{'x0': X0, 'xg': XG}, None, None, YY, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Feed-forward NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the FFNet model\n",
    "FFNet_reg = Regression(prob_features)\n",
    "n_features = 6 # the dimension of feature (input vector)\n",
    "FFNet_reg.setup_data(n_features, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 1000] val loss: 0.0576 | bitwise acc: 0.9736\n",
      "[Iter 2000] val loss: 0.0411 | bitwise acc: 0.9824\n",
      "[Iter 3000] val loss: 0.0387 | bitwise acc: 0.9840\n",
      "[Iter 4000] val loss: 0.0390 | bitwise acc: 0.9828\n",
      "[Iter 5000] val loss: 0.0348 | bitwise acc: 0.9854\n",
      "[Iter 6000] val loss: 0.0375 | bitwise acc: 0.9835\n",
      "[Iter 7000] val loss: 0.0333 | bitwise acc: 0.9856\n",
      "[Iter 8000] val loss: 0.0315 | bitwise acc: 0.9869\n",
      "[Iter 9000] val loss: 0.0323 | bitwise acc: 0.9866\n",
      "[Iter 10000] val loss: 0.0415 | bitwise acc: 0.9817\n",
      "[Iter 10000] Saved model at /home/vietanhle/github/Python/SS_Learning_MIQP/self_supervised/regression_20250716_1515.pt\n",
      "[Iter 11000] val loss: 0.0346 | bitwise acc: 0.9852\n",
      "[Iter 12000] val loss: 0.0316 | bitwise acc: 0.9865\n",
      "[Iter 13000] val loss: 0.0295 | bitwise acc: 0.9877\n",
      "[Iter 14000] val loss: 0.0296 | bitwise acc: 0.9877\n",
      "[Iter 15000] val loss: 0.0319 | bitwise acc: 0.9864\n",
      "[Iter 16000] val loss: 0.0353 | bitwise acc: 0.9844\n",
      "[Iter 17000] val loss: 0.0280 | bitwise acc: 0.9886\n",
      "[Iter 18000] val loss: 0.0274 | bitwise acc: 0.9889\n",
      "[Iter 19000] val loss: 0.0316 | bitwise acc: 0.9864\n",
      "[Iter 20000] val loss: 0.0290 | bitwise acc: 0.9880\n",
      "[Iter 20000] Saved model at /home/vietanhle/github/Python/SS_Learning_MIQP/self_supervised/regression_20250716_1515.pt\n",
      "[Iter 21000] val loss: 0.0272 | bitwise acc: 0.9891\n",
      "[Iter 22000] val loss: 0.0296 | bitwise acc: 0.9878\n",
      "[Iter 23000] val loss: 0.0270 | bitwise acc: 0.9891\n",
      "[Iter 24000] val loss: 0.0311 | bitwise acc: 0.9865\n",
      "[Iter 25000] val loss: 0.0301 | bitwise acc: 0.9875\n",
      "[Iter 26000] val loss: 0.0277 | bitwise acc: 0.9881\n",
      "[Iter 27000] val loss: 0.0281 | bitwise acc: 0.9882\n",
      "[Iter 28000] val loss: 0.0268 | bitwise acc: 0.9890\n",
      "[Iter 29000] val loss: 0.0287 | bitwise acc: 0.9879\n",
      "[Iter 30000] val loss: 0.0295 | bitwise acc: 0.9875\n",
      "[Iter 30000] Saved model at /home/vietanhle/github/Python/SS_Learning_MIQP/self_supervised/regression_20250716_1515.pt\n",
      "[Iter 31000] val loss: 0.0272 | bitwise acc: 0.9888\n",
      "[Iter 32000] val loss: 0.0299 | bitwise acc: 0.9873\n",
      "[Iter 33000] val loss: 0.0283 | bitwise acc: 0.9882\n",
      "Early stopping: no improvement for 10 checks\n",
      "Final model saved at /home/vietanhle/github/Python/SS_Learning_MIQP/self_supervised/regression_20250716_1515.pt\n"
     ]
    }
   ],
   "source": [
    "training_params = {}\n",
    "training_params['TRAINING_EPOCHS'] = int(2000)\n",
    "training_params['BATCH_SIZE'] = 100\n",
    "training_params['CHECKPOINT_AFTER'] = int(1e3)\n",
    "training_params['SAVEPOINT_AFTER'] = int(1e4)\n",
    "training_params['TEST_BATCH_SIZE'] = 100\n",
    "training_params['LEARNING_RATE'] = 1e-3\n",
    "training_params['WEIGHT_DECAY'] = 1e-4\n",
    "training_params['EARLY_STOPPING_PATIENCE'] = 10\n",
    "FFNet_reg.setup_network(depth=4, neurons=1024)\n",
    "FFNet_reg.model\n",
    "FFNet_reg.train(training_params, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
