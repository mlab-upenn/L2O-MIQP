{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import wandb\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) # Adds the parent folder to sys.path\n",
    "from utils import *\n",
    "from model import *\n",
    "# from Neural_Nets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint_violation(dis_traj, cont_traj):\n",
    "    \"\"\"\n",
    "    dis_traj: N_obs x 4 x H array\n",
    "    cont_traj: 2 x H array only for positions\n",
    "    some other parameters such as obstacle info are hard-coded for now\n",
    "    \"\"\"\n",
    "    bigM = 1e3\n",
    "    d_min = 0.25\n",
    "    Obs_info = np.array([[1.0, 1.5, 0.8, 1.6, 0.], [3.0, 2.0, 1.8, 0.6, np.pi/2]])\n",
    "    N_obs = Obs_info.shape[0]\n",
    "    violation = 0.0\n",
    "    for o in range(N_obs):\n",
    "        xo, yo, L0, W0, th = Obs_info[o,:]\n",
    "        L0 = L0/2 + d_min; W0 = W0/2 + d_min\n",
    "        # 4 constraints\n",
    "        c1 = np.maximum(0, np.cos(th)*(cont_traj[0, 1:]-xo) + np.sin(th)*(cont_traj[1, 1:]-yo) - L0 - bigM*(1-dis_traj[o,0,:]))\n",
    "        c2 = np.maximum(0, -np.sin(th)*(cont_traj[0, 1:]-xo) + np.cos(th)*(cont_traj[1, 1:]-yo) - W0 - bigM*(1-dis_traj[o,1,:]))\n",
    "        c3 = np.maximum(0, -np.cos(th)*(cont_traj[0, 1:]-xo) - np.sin(th)*(cont_traj[1, 1:]-yo) - L0 - bigM*(1-dis_traj[o,2,:]))\n",
    "        c4 = np.maximum(0, np.sin(th)*(cont_traj[0, 1:]-xo) - np.cos(th)*(cont_traj[1, 1:]-yo) - W0 - bigM*(1-dis_traj[o,3,:]))\n",
    "        violation += np.sum(c1 + c2 + c3 + c4)\n",
    "    return violation\n",
    "\n",
    "# torch version of the function above\n",
    "def constraint_violation_torch(dis_traj, cont_traj):\n",
    "    \"\"\"\n",
    "    dis_traj: (N_obs, 4, H) tensor of discrete decision variables\n",
    "    cont_traj: (2, H) tensor of continuous variables (positions)\n",
    "    Returns: scalar tensor representing negative constraint violation\n",
    "    \"\"\"\n",
    "    device = cont_traj.device\n",
    "    bigM = 1e3; d_min = 0.25\n",
    "    Obs_info = torch.tensor([\n",
    "        [1.0, 1.5, 0.8, 1.6, 0.],\n",
    "        [3.0, 2.0, 1.8, 0.6, torch.pi/2]\n",
    "    ], device=device)\n",
    "\n",
    "    violation = 0.0\n",
    "    for o in range(Obs_info.size(0)):\n",
    "        xo, yo, L0, W0, th = Obs_info[o]\n",
    "        L0 = L0 / 2 + d_min; W0 = W0 / 2 + d_min\n",
    "        x = cont_traj[0, 1:]; y = cont_traj[1, 1:]\n",
    "        cos_th = torch.cos(th); sin_th = torch.sin(th)\n",
    "        d = dis_traj[o] # (4, H)\n",
    "        c1 = torch.relu(cos_th * (x - xo) + sin_th * (y - yo) - L0 - bigM * (1 - d[0, :]))\n",
    "        c2 = torch.relu(-sin_th * (x - xo) + cos_th * (y - yo) - W0 - bigM * (1 - d[1, :]))\n",
    "        c3 = torch.relu(-cos_th * (x - xo) - sin_th * (y - yo) - L0 - bigM * (1 - d[2, :]))\n",
    "        c4 = torch.relu(sin_th * (x - xo) - cos_th * (y - yo) - W0 - bigM * (1 - d[3, :]))\n",
    "        c5 = torch.relu(1 - d.sum(dim=0))\n",
    "\n",
    "        violation += torch.sum(c1 + c2 + c3 + c4 + c5)\n",
    "\n",
    "    return violation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "\n",
    "    def __init__(self, prob_features):\n",
    "        \"\"\"\n",
    "        Constructor for Regression class.\n",
    "        \"\"\"\n",
    "        self.prob_features = prob_features\n",
    "        self.num_train, self.num_test = 0, 0\n",
    "        self.model, self.model_fn = None, None\n",
    "        self.n_bin = 4 # number of binaries used in the collision avoidance constraint\n",
    "        self.n_obs = 2 # number of obstacles\n",
    "\n",
    "    def construct_features(self, params):\n",
    "        prob_features = self.prob_features\n",
    "        feature_vec = np.array([])\n",
    "        for feature in prob_features:\n",
    "            if feature == \"x0\":\n",
    "                x0 = params['x0']\n",
    "                feature_vec = np.hstack((feature_vec, x0))\n",
    "            elif feature == \"xg\":\n",
    "                xg = params['xg'] \n",
    "                feature_vec = np.hstack((feature_vec, xg))\n",
    "            elif feature == \"obstacles\":\n",
    "                obstacles = params['obstacles']\n",
    "                feature_vec = np.hstack((feature_vec, np.reshape(obstacles, (4*self.n_obs))))\n",
    "            elif feature == \"obstacles_map\":\n",
    "                continue\n",
    "            else:\n",
    "                print('Feature {} is unknown'.format(feature))\n",
    "        return feature_vec\n",
    "\n",
    "    def setup_data(self, n_features, train_data):\n",
    "        \"\"\"\n",
    "        Reads in data and constructs strategy dictionary\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.X_train = train_data[0] # Problem parameters, will be inputs of the NNs\n",
    "        self.Y_train = train_data[2] # Discrete solutions, will be outputs of the NNs\n",
    "        self.P_train = train_data[1] # Continuous trajectories, will be used as parameters in training\n",
    "        self.n_y = self.Y_train[0].size # will be the dimension of the output\n",
    "        self.y_shape = self.Y_train[0].shape\n",
    "        self.num_train = self.Y_train.shape[0]        \n",
    "\n",
    "        # Create features and labels based on raw data\n",
    "        self.features = np.zeros((self.num_train, self.n_features))\n",
    "        self.labels = np.zeros((self.num_train, self.n_y))\n",
    "        self.outputs = np.zeros((self.num_train, self.n_y*self.n_bin))        \n",
    "        for ii in range(self.num_train):\n",
    "            self.labels[ii] = np.reshape(self.Y_train[ii,:,:], (self.n_y))\n",
    "            self.outputs[ii] = np.hstack([int_to_four_bins(val) for val in (self.labels[ii])])\n",
    "            prob_params = {}\n",
    "            for k in self.X_train:\n",
    "                prob_params[k] = self.X_train[k][ii]\n",
    "            self.features[ii] = self.construct_features(prob_params)\n",
    "\n",
    "    def setup_network(self, depth=3, neurons=32, device_id=0):\n",
    "        self.device = torch.device('cuda:{}'.format(device_id))\n",
    "        ff_shape = [self.n_features]\n",
    "        for ii in range(depth):\n",
    "            ff_shape.append(neurons)\n",
    "        ff_shape.append(self.n_y*self.n_bin)\n",
    "\n",
    "        self.model = FFNet(ff_shape, activation=torch.nn.ReLU()).to(device=self.device)\n",
    "\n",
    "        # file names for PyTorch models\n",
    "        now = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "        model_fn = 'regression_{}.pt'\n",
    "        model_fn = os.path.join(os.getcwd(), model_fn)\n",
    "        self.model_fn = model_fn.format(now)\n",
    "\n",
    "    def load_network(self, fn_classifier_model):\n",
    "        if os.path.exists(fn_classifier_model):\n",
    "            print('Loading presaved Hetero GNN classifier model from {}'.format(fn_classifier_model))\n",
    "            self.model.load_state_dict(torch.load(fn_classifier_model))\n",
    "            self.model_fn = fn_classifier_model\n",
    "\n",
    "    def train(self, training_params, verbose=True):\n",
    "        BATCH_SIZE = training_params['BATCH_SIZE']\n",
    "        TEST_BATCH_SIZE = training_params['TEST_BATCH_SIZE']\n",
    "        TRAINING_EPOCHS = training_params['TRAINING_EPOCHS']\n",
    "        CHECKPOINT_AFTER = training_params['CHECKPOINT_AFTER']\n",
    "        SAVEPOINT_AFTER = training_params['SAVEPOINT_AFTER']\n",
    "        LEARNING_RATE = training_params['LEARNING_RATE']\n",
    "        WEIGHT_DECAY = training_params['WEIGHT_DECAY']\n",
    "        EARLY_STOPPING_PATIENCE = training_params['EARLY_STOPPING_PATIENCE']\n",
    "\n",
    "        model = self.model\n",
    "        device = self.device\n",
    "\n",
    "        # Initialize wandb\n",
    "        wandb.init(project=\"Learning_MICP\", config=training_params)        \n",
    "\n",
    "        # Prepare dataset\n",
    "        X_tensor = torch.from_numpy(self.features).float()\n",
    "        Y_tensor = torch.from_numpy(self.outputs).float()\n",
    "        P_tensor = torch.from_numpy(self.P_train['XX'][:,:2,:]).float()\n",
    "        full_dataset = TensorDataset(X_tensor, Y_tensor, P_tensor)\n",
    "\n",
    "        # Split into train/validation\n",
    "        num_total = len(full_dataset)\n",
    "        num_train = int(0.9*num_total)\n",
    "        train_dataset = TensorDataset(X_tensor[:num_train], Y_tensor[:num_train], P_tensor[:num_train])\n",
    "        val_dataset   = TensorDataset(X_tensor[num_train:], Y_tensor[num_train:], P_tensor[num_train:])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_since_improvement = 0\n",
    "\n",
    "        itr = 1\n",
    "        for epoch in range(TRAINING_EPOCHS):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, y_true, _ in train_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                y_true = y_true.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, y_true)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                wandb.log({\"train_loss\": loss.item(), \"iteration\": itr})\n",
    "                itr += 1\n",
    "\n",
    "            avg_train_loss = running_loss / len(train_loader)\n",
    "            # Log to wandb\n",
    "            wandb.log({\"avg_train_loss\": avg_train_loss, \"epoch\": epoch})\n",
    "            \n",
    "            if epoch % SAVEPOINT_AFTER == 0:\n",
    "                torch.save(model.state_dict(), self.model_fn)\n",
    "                if verbose:\n",
    "                    print(f\"[Epoch {epoch}], [Iter {itr}] Saved model at {self.model_fn}\")\n",
    "\n",
    "            if epoch % CHECKPOINT_AFTER == 0:\n",
    "                # Evaluate on validation set\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss_total = 0\n",
    "                    val_cons_violation = []\n",
    "                    bitwise_accs = []\n",
    "\n",
    "                    for val_inputs, val_targets, val_params in val_loader:\n",
    "                        # Get the loss values\n",
    "                        val_inputs = val_inputs.to(device)\n",
    "                        val_targets = val_targets.to(device)\n",
    "                        val_params = val_params.to(device)\n",
    "                        val_logits = model(val_inputs)\n",
    "                        val_loss = loss_fn(val_logits, val_targets)\n",
    "                        val_loss_total += val_loss.item()\n",
    "\n",
    "                        val_preds = val_logits.int() # Already rounded by STE_Round\n",
    "                        # Compare accuracy\n",
    "                        bitwise_accs.append(compute_bitwise_accuracy(val_preds, val_targets.int()))\n",
    "\n",
    "                        # Evaluate constraint violation\n",
    "                        constraint_loss = self.batch_constraint_violation_loss(val_preds, val_params).item()\n",
    "                        val_cons_violation.append(constraint_loss)\n",
    "\n",
    "                    avg_val_loss = val_loss_total/len(val_loader)\n",
    "                    avg_bitwise_acc = np.mean(bitwise_accs)\n",
    "                    avg_val_cons_violation = np.mean(val_cons_violation)\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"[Epoch {epoch}], [Iter {itr}] Validation loss: {avg_val_loss:.4f} | \"\n",
    "                            f\"Validation accuracy (bitwise): {avg_bitwise_acc:.4f} | \"\n",
    "                            f\"Constraint violation: {avg_val_cons_violation:.4f}\")\n",
    "                    \n",
    "                    # Log to wandb\n",
    "                    wandb.log({\"val/loss\": avg_val_loss,\n",
    "                        \"val/bitwise_acc\": avg_bitwise_acc,\n",
    "                        \"val/constraint_violation\": avg_val_cons_violation,\n",
    "                        \"epoch\": epoch})\n",
    "\n",
    "                    # Check for early stopping\n",
    "                    if avg_val_loss < best_val_loss - 1e-3:\n",
    "                        best_val_loss = avg_val_loss\n",
    "                        epochs_since_improvement = 0\n",
    "                    else:\n",
    "                        epochs_since_improvement += 1\n",
    "                        if epochs_since_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "                            print(f\"Early stopping: no improvement for {EARLY_STOPPING_PATIENCE} epochs\")\n",
    "                            torch.save(model.state_dict(), self.model_fn)\n",
    "                            wandb.save(self.model_fn)\n",
    "                            print(f\"Final model saved at {self.model_fn}\")\n",
    "                            wandb.finish()\n",
    "                            return  # Exit training early\n",
    "\n",
    "        # Save final model\n",
    "        torch.save(model.state_dict(), self.model_fn)\n",
    "        wandb.save(self.model_fn)\n",
    "        print(f\"Final model saved at {self.model_fn}\")\n",
    "        print(\"Done training.\")\n",
    "        wandb.finish()\n",
    "\n",
    "    def batch_constraint_violation_loss(self, dis_traj_pred, cont_traj_pred, lambda_penalty=1.0):\n",
    "        \"\"\"\n",
    "        Compute the constraint violation as loss function for a batch data\n",
    "        \"\"\"\n",
    "        batch_size = dis_traj_pred.size(0)\n",
    "        total_violation = 0.0\n",
    "        for b in range(batch_size):\n",
    "            dis_traj = NNoutput_reshape_torch(dis_traj_pred[b], self.n_obs)\n",
    "            cont_traj = cont_traj_pred[b]\n",
    "            total_violation += constraint_violation_torch(dis_traj, cont_traj)\n",
    "        return lambda_penalty * total_violation / batch_size\n",
    "\n",
    "    # Train with self-supervised loss function\n",
    "    def SS_train(self, training_params, verbose=True, penalty_weight = 1.0):\n",
    "        \"\"\"\n",
    "        Implement self-supervised learning with constraint violation based loss\n",
    "        penalty_weight: the penalty weight for constraint violation if linearly combine with supervised loss \n",
    "        \"\"\"\n",
    "        BATCH_SIZE = training_params['BATCH_SIZE']\n",
    "        TEST_BATCH_SIZE = training_params['TEST_BATCH_SIZE']\n",
    "        TRAINING_EPOCHS = training_params['TRAINING_EPOCHS']\n",
    "        CHECKPOINT_AFTER = training_params['CHECKPOINT_AFTER']\n",
    "        SAVEPOINT_AFTER = training_params['SAVEPOINT_AFTER']\n",
    "        LEARNING_RATE = training_params['LEARNING_RATE']\n",
    "        WEIGHT_DECAY = training_params['WEIGHT_DECAY']\n",
    "        EARLY_STOPPING_PATIENCE = training_params['EARLY_STOPPING_PATIENCE']\n",
    "\n",
    "        model = self.model\n",
    "        device = self.device\n",
    "\n",
    "        wandb.init(project=\"Learning_MICP\", config=training_params)        \n",
    "\n",
    "        # Prepare dataset\n",
    "        X_tensor = torch.from_numpy(self.features).float()\n",
    "        Y_tensor = torch.from_numpy(self.outputs).float()\n",
    "        P_tensor = torch.from_numpy(self.P_train['XX'][:,:2,:]).float()\n",
    "        full_dataset = TensorDataset(X_tensor, Y_tensor, P_tensor)\n",
    "\n",
    "        # Split into train/val\n",
    "        num_total = len(full_dataset)\n",
    "        num_train = int(0.9*num_total)\n",
    "        train_dataset = TensorDataset(X_tensor[:num_train], Y_tensor[:num_train], P_tensor[:num_train])\n",
    "        val_dataset = TensorDataset(X_tensor[num_train:], Y_tensor[num_train:], P_tensor[num_train:])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        # supervised loss and optimizer\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)        \n",
    "        best_val_loss = float('inf')\n",
    "        epochs_since_improvement = 0\n",
    "\n",
    "        itr = 1\n",
    "        for epoch in range(TRAINING_EPOCHS):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, y_true, params in train_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                y_true = y_true.to(device)\n",
    "                params = params.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(inputs)  # shape: (B, N_obs*4*H)\n",
    "                loss = self.batch_constraint_violation_loss(logits, params, lambda_penalty=penalty_weight) # + loss_fn(logits, y_true)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                wandb.log({\"train_loss\": loss.item(), \"iteration\": itr})\n",
    "                itr += 1\n",
    "\n",
    "            avg_train_loss = running_loss / len(train_loader)\n",
    "            # Log to wandb\n",
    "            wandb.log({\"avg_train_loss\": avg_train_loss, \"epoch\": epoch})\n",
    "\n",
    "            if epoch % SAVEPOINT_AFTER == 0:\n",
    "                torch.save(model.state_dict(), self.model_fn)\n",
    "                if verbose:\n",
    "                    print(f\"[Epoch {epoch}], [Iter {itr}] Saved model at {self.model_fn}\")\n",
    "\n",
    "            if epoch % CHECKPOINT_AFTER == 0:\n",
    "                # Evaluate on validation set\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss_total = 0\n",
    "                    val_cons_violation = []\n",
    "                    bitwise_accs = []\n",
    "\n",
    "                    for val_inputs, val_targets, val_params in val_loader:\n",
    "                        val_inputs = val_inputs.to(device)\n",
    "                        val_targets = val_targets.to(device)\n",
    "                        val_params = val_params.to(device)\n",
    "\n",
    "                        val_logits = model(val_inputs)\n",
    "                        val_loss = loss_fn(val_logits, val_targets) + self.batch_constraint_violation_loss(val_logits, val_params, lambda_penalty=penalty_weight)\n",
    "                        val_loss_total += val_loss.item()\n",
    "                        val_preds = val_logits.int() # Already rounded by STE_Round\n",
    "\n",
    "                        # Compare accuracy\n",
    "                        bitwise_accs.append(compute_bitwise_accuracy(val_preds, val_targets.int()))\n",
    "                        # Evaluate constraint violation\n",
    "                        constraint_loss = self.batch_constraint_violation_loss(val_preds, val_params).item()\n",
    "                        val_cons_violation.append(constraint_loss)\n",
    "\n",
    "                    avg_val_loss = val_loss_total/len(val_loader)\n",
    "                    avg_bitwise_acc = np.mean(bitwise_accs)\n",
    "                    avg_val_cons_violation = np.mean(val_cons_violation)\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"[Epoch {epoch}], [Iter {itr}] Validation loss: {avg_val_loss:.4f} | \"\n",
    "                            f\"Validation accuracy (bitwise): {avg_bitwise_acc:.4f} | \"\n",
    "                            f\"Constraint violation: {avg_val_cons_violation:.4f}\")\n",
    "                    # Log to wandb\n",
    "                    wandb.log({\"val/loss\": avg_val_loss,\n",
    "                        \"val/bitwise_acc\": avg_bitwise_acc,\n",
    "                        \"val/constraint_violation\": avg_val_cons_violation,\n",
    "                        \"epoch\": epoch})\n",
    "                    \n",
    "                    # Check for early stopping\n",
    "                    if avg_val_loss < best_val_loss - 1e-3:\n",
    "                        best_val_loss = avg_val_loss\n",
    "                        epochs_since_improvement = 0\n",
    "                    else:\n",
    "                        epochs_since_improvement += 1\n",
    "                        if epochs_since_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "                            print(f\"Early stopping: no improvement for {EARLY_STOPPING_PATIENCE} epochs\")\n",
    "                            torch.save(model.state_dict(), self.model_fn)\n",
    "                            wandb.save(self.model_fn)\n",
    "                            print(f\"Final model saved at {self.model_fn}\")\n",
    "                            wandb.finish()\n",
    "                            return  # Exit training early\n",
    "\n",
    "        # Save final model\n",
    "        torch.save(model.state_dict(), self.model_fn)\n",
    "        wandb.save(self.model_fn)\n",
    "        print(f\"Final model saved at {self.model_fn}\")\n",
    "        print(\"Done training.\")\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 137470\n"
     ]
    }
   ],
   "source": [
    "relative_path = os.getcwd()\n",
    "relative_path = os.path.abspath(\"..\")\n",
    "dataset_fn = relative_path + '/data' + '/single.p'\n",
    "prob_features = ['x0', 'xg']\n",
    "\n",
    "data_file = open(dataset_fn,'rb')\n",
    "all_data = pickle.load(data_file)\n",
    "data_file.close()\n",
    "num_train = len(all_data)\n",
    "print(f\"Number of training samples: {num_train}\")\n",
    "\n",
    "X0 = np.vstack([all_data[ii]['x0'].T for ii in range(num_train)])  \n",
    "XG = np.vstack([all_data[ii]['xg'].T for ii in range(num_train)])  \n",
    "OBS = np.vstack([all_data[ii]['xg'].T for ii in range(num_train)])  \n",
    "XX = np.array([all_data[ii]['XX'] for ii in range(num_train)])\n",
    "UU = np.array([all_data[ii]['UU'] for ii in range(num_train)])\n",
    "YY = np.concatenate([all_data[ii]['YY'].astype(int) for ii in range(num_train)], axis=1).transpose(1,0,2)\n",
    "train_data = [{'x0': X0, 'xg': XG}, {'XX': XX, 'UU' : UU}, YY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Regression with Feed-forward NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the FFNet model\n",
    "FFNet_reg = Regression(prob_features)\n",
    "n_features = 6 # the dimension of feature (input vector)\n",
    "FFNet_reg.setup_data(n_features, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNet_reg.setup_network(depth=4, neurons=1024)\n",
    "FFNet_reg.model\n",
    "\n",
    "training_params = {}\n",
    "training_params['TRAINING_EPOCHS'] = int(1000)\n",
    "training_params['BATCH_SIZE'] = 200\n",
    "training_params['CHECKPOINT_AFTER'] = int(1)\n",
    "training_params['SAVEPOINT_AFTER'] = int(10)\n",
    "training_params['TEST_BATCH_SIZE'] = 100\n",
    "training_params['LEARNING_RATE'] = 1e-3\n",
    "training_params['WEIGHT_DECAY'] = 1e-5\n",
    "training_params['EARLY_STOPPING_PATIENCE'] = 5\n",
    "\n",
    "FFNet_reg.train(training_params, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Refine with Self-Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading presaved Hetero GNN classifier model from regression_20251007_1324.pt\n",
      "FFNet(\n",
      "  (activation): ReLU()\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=6, out_features=1024, bias=True)\n",
      "    (1-3): 3 x Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (4): Linear(in_features=1024, out_features=160, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "fn_saved = 'regression_20251007_1324.pt'  \n",
    "FFNet_reg.setup_network(depth=4, neurons=1024)\n",
    "FFNet_reg.load_network(fn_saved)\n",
    "print(FFNet_reg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvietanh_penn\u001b[0m (\u001b[33mvietanh_penn-university-of-pennsylvania\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu22/github/UPenn_Py/SS_Learning_MIQP/self_supervised/wandb/run-20251007_133444-kozbaiqe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vietanh_penn-university-of-pennsylvania/Learning_MICP/runs/kozbaiqe' target=\"_blank\">prime-dragon-20</a></strong> to <a href='https://wandb.ai/vietanh_penn-university-of-pennsylvania/Learning_MICP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vietanh_penn-university-of-pennsylvania/Learning_MICP' target=\"_blank\">https://wandb.ai/vietanh_penn-university-of-pennsylvania/Learning_MICP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vietanh_penn-university-of-pennsylvania/Learning_MICP/runs/kozbaiqe' target=\"_blank\">https://wandb.ai/vietanh_penn-university-of-pennsylvania/Learning_MICP/runs/kozbaiqe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0], [Iter 620] Saved model at regression_20251007_1324.pt\n",
      "[Epoch 0], [Iter 620] Validation loss: 0.5507 | Validation accuracy (bitwise): 0.7891 | Constraint violation: 0.0000\n",
      "[Epoch 1], [Iter 1239] Validation loss: 0.5507 | Validation accuracy (bitwise): 0.7891 | Constraint violation: 0.0000\n",
      "[Epoch 2], [Iter 1858] Validation loss: 0.5507 | Validation accuracy (bitwise): 0.7890 | Constraint violation: 0.0000\n",
      "[Epoch 3], [Iter 2477] Validation loss: 0.5507 | Validation accuracy (bitwise): 0.7891 | Constraint violation: 0.0000\n",
      "[Epoch 4], [Iter 3096] Validation loss: 0.5507 | Validation accuracy (bitwise): 0.7891 | Constraint violation: 0.0000\n",
      "[Epoch 5], [Iter 3715] Validation loss: 0.5507 | Validation accuracy (bitwise): 0.7891 | Constraint violation: 0.0000\n",
      "Early stopping: no improvement for 5 epochs\n",
      "Final model saved at regression_20251007_1324.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>iteration</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/bitwise_acc</td><td>▅█▁▃▅▆</td></tr><tr><td>val/constraint_violation</td><td>▁▁▁▁▁▁</td></tr><tr><td>val/loss</td><td>▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.00729</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>iteration</td><td>3714</td></tr><tr><td>train_loss</td><td>0</td></tr><tr><td>val/bitwise_acc</td><td>0.78908</td></tr><tr><td>val/constraint_violation</td><td>0</td></tr><tr><td>val/loss</td><td>0.55069</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prime-dragon-20</strong> at: <a href='https://wandb.ai/vietanh_penn-university-of-pennsylvania/Learning_MICP/runs/kozbaiqe' target=\"_blank\">https://wandb.ai/vietanh_penn-university-of-pennsylvania/Learning_MICP/runs/kozbaiqe</a><br> View project at: <a href='https://wandb.ai/vietanh_penn-university-of-pennsylvania/Learning_MICP' target=\"_blank\">https://wandb.ai/vietanh_penn-university-of-pennsylvania/Learning_MICP</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251007_133444-kozbaiqe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_params = {}\n",
    "training_params['TRAINING_EPOCHS'] = int(1000)\n",
    "training_params['BATCH_SIZE'] = 200\n",
    "training_params['CHECKPOINT_AFTER'] = int(1)\n",
    "training_params['SAVEPOINT_AFTER'] = int(10)\n",
    "training_params['TEST_BATCH_SIZE'] = 100\n",
    "training_params['LEARNING_RATE'] = 1e-3\n",
    "training_params['WEIGHT_DECAY'] = 1e-4\n",
    "training_params['EARLY_STOPPING_PATIENCE'] = 5\n",
    "\n",
    "FFNet_reg.SS_train(training_params, verbose=True, penalty_weight=1e0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penn_env312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
