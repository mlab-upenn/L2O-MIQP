{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) # Adds the parent folder to sys.path\n",
    "from utils import *\n",
    "from Neural_Nets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert an integer to 4 binaries (bits)\n",
    "def int_to_four_bins(n):\n",
    "    return np.array(list(np.binary_repr(int(n), width=4))).astype(int)\n",
    "\n",
    "# This function computes the average accuracy per binary\n",
    "def compute_bitwise_accuracy(preds, targets):\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "# This function computes the strict accuracy: it counts how many entire binary vectors are predicted exactly right\n",
    "def compute_exact_match_accuracy(preds, targets):\n",
    "    return torch.all(preds == targets, dim=1).float().mean().item()\n",
    "\n",
    "def NNoutput_reshape(outputs, N_obs):\n",
    "    dis_traj_temp = outputs.reshape([N_obs,-1], order='C')\n",
    "    dis_traj = np.array([dis_traj_temp[i].reshape([4,-1], order='F') for i in range(2)])\n",
    "    return dis_traj\n",
    "\n",
    "# torch version of the function above\n",
    "def NNoutput_reshape_torch(outputs: torch.Tensor, N_obs: int):\n",
    "    if outputs.dim() == 1:\n",
    "        outputs = outputs.view(N_obs, -1) # (N_obs, 4*H)\n",
    "    H = outputs.shape[1] // 4\n",
    "    # reshape to (N_obs, H, 4) in C-order\n",
    "    dis_traj = outputs.view(N_obs, H, 4)\n",
    "    # transpose to (N_obs, 4, H) to match NumPy's F-order reshape\n",
    "    dis_traj = dis_traj.transpose(1, 2).contiguous()\n",
    "    return dis_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint_violation(dis_traj, cont_traj):\n",
    "    \"\"\"\n",
    "    dis_traj: N_obs x 4 x H array\n",
    "    cont_traj: 2 x H array only for positions\n",
    "    some other parameters such as obstacle info are hard-coded for now\n",
    "    \"\"\"\n",
    "    bigM = 1e3\n",
    "    d_min = 0.25\n",
    "    Obs_info = np.array([[1.0, 1.5, 0.8, 1.6, 0.], [3.0, 2.0, 1.8, 0.6, np.pi/2]])\n",
    "    N_obs = Obs_info.shape[0]\n",
    "    violation = 0.0\n",
    "    for o in range(N_obs):\n",
    "        xo, yo, L0, W0, th = Obs_info[o,:]\n",
    "        L0 = L0/2 + d_min; W0 = W0/2 + d_min\n",
    "        # 4 constraints\n",
    "        c1 = np.maximum(0, np.cos(th)*(cont_traj[0, 1:]-xo) + np.sin(th)*(cont_traj[1, 1:]-yo) - L0 - bigM*(1-dis_traj[o,0,:]))\n",
    "        c2 = np.maximum(0, -np.sin(th)*(cont_traj[0, 1:]-xo) + np.cos(th)*(cont_traj[1, 1:]-yo) - W0 - bigM*(1-dis_traj[o,1,:]))\n",
    "        c3 = np.maximum(0, -np.cos(th)*(cont_traj[0, 1:]-xo) - np.sin(th)*(cont_traj[1, 1:]-yo) - L0 - bigM*(1-dis_traj[o,2,:]))\n",
    "        c4 = np.maximum(0, np.sin(th)*(cont_traj[0, 1:]-xo) - np.cos(th)*(cont_traj[1, 1:]-yo) - W0 - bigM*(1-dis_traj[o,3,:]))\n",
    "        violation += np.sum(c1 + c2 + c3 + c4)\n",
    "    return violation\n",
    "\n",
    "# torch version of the function above\n",
    "def constraint_violation_torch(dis_traj, cont_traj):\n",
    "    \"\"\"\n",
    "    dis_traj: (N_obs, 4, H) tensor of discrete decision variables\n",
    "    cont_traj: (2, H) tensor of continuous variables (positions)\n",
    "    Returns: scalar tensor representing negative constraint violation\n",
    "    \"\"\"\n",
    "    device = cont_traj.device\n",
    "    bigM = 1e3; d_min = 0.25\n",
    "    Obs_info = torch.tensor([\n",
    "        [1.0, 1.5, 0.8, 1.6, 0.],\n",
    "        [3.0, 2.0, 1.8, 0.6, torch.pi/2]\n",
    "    ], device=device)\n",
    "\n",
    "    violation = 0.0\n",
    "    for o in range(Obs_info.size(0)):\n",
    "        xo, yo, L0, W0, th = Obs_info[o]\n",
    "        L0 = L0 / 2 + d_min; W0 = W0 / 2 + d_min\n",
    "        x = cont_traj[0, 1:]; y = cont_traj[1, 1:]\n",
    "        cos_th = torch.cos(th); sin_th = torch.sin(th)\n",
    "        d = dis_traj[o] # (4, H)\n",
    "        c1 = torch.relu(cos_th * (x - xo) + sin_th * (y - yo) - L0 - bigM * (1 - d[0, :]))\n",
    "        c2 = torch.relu(-sin_th * (x - xo) + cos_th * (y - yo) - W0 - bigM * (1 - d[1, :]))\n",
    "        c3 = torch.relu(-cos_th * (x - xo) - sin_th * (y - yo) - L0 - bigM * (1 - d[2, :]))\n",
    "        c4 = torch.relu(sin_th * (x - xo) - cos_th * (y - yo) - W0 - bigM * (1 - d[3, :]))\n",
    "        c5 = torch.relu(1 - d.sum(dim=0))\n",
    "\n",
    "        violation += torch.sum(c1 + c2 + c3 + c4 + c5)\n",
    "\n",
    "    return violation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STE_Round(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return torch.round(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "class FFNet(torch.nn.Module):\n",
    "    \"\"\"Feed-forward neural network with optional activation and STE rounding at the output.\"\"\"\n",
    "    \n",
    "    def __init__(self, shape, activation=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            shape: list of ints describing network shape, including input & output size.\n",
    "            activation: a torch.nn function specifying the network activation (e.g., torch.nn.ReLU()).\n",
    "        \"\"\"\n",
    "        super(FFNet, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.activation = activation\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(len(shape) - 1):\n",
    "            self.layers.append(torch.nn.Linear(shape[i], shape[i + 1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.layers[i](x)\n",
    "            if self.activation is not None:\n",
    "                x = self.activation(x)\n",
    "        \n",
    "        x = self.layers[-1](x)\n",
    "        x = torch.sigmoid(x) \n",
    "        return STE_Round.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "\n",
    "    def __init__(self, prob_features):\n",
    "        \"\"\"\n",
    "        Constructor for Regression class.\n",
    "        \"\"\"\n",
    "        self.prob_features = prob_features\n",
    "        self.num_train, self.num_test = 0, 0\n",
    "        self.model, self.model_fn = None, None\n",
    "        self.n_bin = 4 # number of binaries used in the collision avoidance constraint\n",
    "        self.n_obs = 2 # number of obstacles\n",
    "\n",
    "    def construct_features(self, params):\n",
    "        prob_features = self.prob_features\n",
    "        feature_vec = np.array([])\n",
    "        for feature in prob_features:\n",
    "            if feature == \"x0\":\n",
    "                x0 = params['x0']\n",
    "                feature_vec = np.hstack((feature_vec, x0))\n",
    "            elif feature == \"xg\":\n",
    "                xg = params['xg'] \n",
    "                feature_vec = np.hstack((feature_vec, xg))\n",
    "            elif feature == \"obstacles\":\n",
    "                obstacles = params['obstacles']\n",
    "                feature_vec = np.hstack((feature_vec, np.reshape(obstacles, (4*self.n_obs))))\n",
    "            elif feature == \"obstacles_map\":\n",
    "                continue\n",
    "            else:\n",
    "                print('Feature {} is unknown'.format(feature))\n",
    "        return feature_vec\n",
    "\n",
    "    def setup_data(self, n_features, train_data):\n",
    "        \"\"\"\n",
    "        Reads in data and constructs strategy dictionary\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.X_train = train_data[0] # Problem parameters, will be inputs of the NNs\n",
    "        self.Y_train = train_data[2] # Discrete solutions, will be outputs of the NNs\n",
    "        self.P_train = train_data[1] # Continuous trajectories, will be used as parameters in training\n",
    "        self.n_y = self.Y_train[0].size # will be the dimension of the output\n",
    "        self.y_shape = self.Y_train[0].shape\n",
    "        self.num_train = self.Y_train.shape[0]        \n",
    "\n",
    "        # Create features and labels based on raw data\n",
    "        self.features = np.zeros((self.num_train, self.n_features))\n",
    "        self.labels = np.zeros((self.num_train, self.n_y))\n",
    "        self.outputs = np.zeros((self.num_train, self.n_y*self.n_bin))        \n",
    "        for ii in range(self.num_train):\n",
    "            self.labels[ii] = np.reshape(self.Y_train[ii,:,:], (self.n_y))\n",
    "            self.outputs[ii] = np.hstack([int_to_four_bins(val) for val in (self.labels[ii])])\n",
    "            prob_params = {}\n",
    "            for k in self.X_train:\n",
    "                prob_params[k] = self.X_train[k][ii]\n",
    "            self.features[ii] = self.construct_features(prob_params)\n",
    "\n",
    "    def setup_network(self, depth=3, neurons=32, device_id=0):\n",
    "        self.device = torch.device('cuda:{}'.format(device_id))\n",
    "        ff_shape = [self.n_features]\n",
    "        for ii in range(depth):\n",
    "            ff_shape.append(neurons)\n",
    "        ff_shape.append(self.n_y*self.n_bin)\n",
    "\n",
    "        self.model = FFNet(ff_shape, activation=torch.nn.ReLU()).to(device=self.device)\n",
    "\n",
    "        # file names for PyTorch models\n",
    "        now = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "        model_fn = 'regression_{}.pt'\n",
    "        model_fn = os.path.join(os.getcwd(), model_fn)\n",
    "        self.model_fn = model_fn.format(now)\n",
    "\n",
    "    def load_network(self, fn_classifier_model):\n",
    "        if os.path.exists(fn_classifier_model):\n",
    "            print('Loading presaved Hetero GNN classifier model from {}'.format(fn_classifier_model))\n",
    "            self.model.load_state_dict(torch.load(fn_classifier_model))\n",
    "            self.model_fn = fn_classifier_model\n",
    "\n",
    "    def train(self, training_params, verbose=True):\n",
    "        BATCH_SIZE = training_params['BATCH_SIZE']\n",
    "        TEST_BATCH_SIZE = training_params['TEST_BATCH_SIZE']\n",
    "        TRAINING_EPOCHS = training_params['TRAINING_EPOCHS']\n",
    "        CHECKPOINT_AFTER = training_params['CHECKPOINT_AFTER']\n",
    "        SAVEPOINT_AFTER = training_params['SAVEPOINT_AFTER']\n",
    "        LEARNING_RATE = training_params['LEARNING_RATE']\n",
    "        WEIGHT_DECAY = training_params['WEIGHT_DECAY']\n",
    "        EARLY_STOPPING_PATIENCE = training_params['EARLY_STOPPING_PATIENCE']\n",
    "\n",
    "        model = self.model\n",
    "        device = self.device\n",
    "\n",
    "        # Prepare dataset\n",
    "        X_tensor = torch.from_numpy(self.features).float()\n",
    "        Y_tensor = torch.from_numpy(self.outputs).float()\n",
    "        P_tensor = torch.from_numpy(self.P_train['XX'][:,:2,:]).float()\n",
    "        full_dataset = TensorDataset(X_tensor, Y_tensor, P_tensor)\n",
    "\n",
    "        # Split into train/validation\n",
    "        num_total = len(full_dataset)\n",
    "        num_train = int(0.9*num_total)\n",
    "        train_dataset = TensorDataset(X_tensor[:num_train], Y_tensor[:num_train], P_tensor[:num_train])\n",
    "        val_dataset   = TensorDataset(X_tensor[num_train:], Y_tensor[num_train:], P_tensor[num_train:])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_since_improvement = 0\n",
    "\n",
    "        itr = 1\n",
    "        for epoch in range(TRAINING_EPOCHS):\n",
    "            model.train()\n",
    "            for inputs, y_true, _ in train_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                y_true = y_true.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(inputs)\n",
    "                loss = loss_fn(logits, y_true)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if itr % CHECKPOINT_AFTER == 0:\n",
    "                    # Evaluate on validation set\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        val_loss_total = 0\n",
    "                        val_cons_violation = []\n",
    "                        bitwise_accs = []\n",
    "\n",
    "                        for val_inputs, val_targets, val_params in val_loader:\n",
    "                            # Get the loss values\n",
    "                            val_inputs = val_inputs.to(device)\n",
    "                            val_targets = val_targets.to(device)\n",
    "                            val_params = val_params.to(device)\n",
    "                            val_logits = model(val_inputs)\n",
    "                            val_loss = loss_fn(val_logits, val_targets)\n",
    "                            val_loss_total += val_loss.item()\n",
    "\n",
    "                            val_preds = val_logits.int() # Already rounded by STE_Round\n",
    "                            # Compare accuracy\n",
    "                            bitwise_accs.append(compute_bitwise_accuracy(val_preds, val_targets.int()))\n",
    "\n",
    "                            # Evaluate constraint violation\n",
    "                            constraint_loss = self.batch_constraint_violation_loss(val_preds, val_params).item()\n",
    "                            val_cons_violation.append(constraint_loss)\n",
    "\n",
    "                        avg_val_loss = val_loss_total/len(val_loader)\n",
    "                        avg_bitwise_acc = np.mean(bitwise_accs)\n",
    "                        avg_val_cons_violation = np.mean(val_cons_violation)\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f\"[Iter {itr}] Validation loss: {avg_val_loss:.4f} | \"\n",
    "                                f\"Validation accuracy (bitwise): {avg_bitwise_acc:.4f} | \"\n",
    "                                f\"Constraint violation: {avg_val_cons_violation:.4f}\")\n",
    "\n",
    "                        # Check for early stopping\n",
    "                        if avg_val_loss < best_val_loss - 1e-3:\n",
    "                            best_val_loss = avg_val_loss\n",
    "                            epochs_since_improvement = 0\n",
    "                        else:\n",
    "                            epochs_since_improvement += 1\n",
    "                            if epochs_since_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "                                print(f\"Early stopping: no improvement for {EARLY_STOPPING_PATIENCE} epoches\")\n",
    "                                torch.save(model.state_dict(), self.model_fn)\n",
    "                                print(f\"Final model saved at {self.model_fn}\")\n",
    "                                return  # Exit training early\n",
    "                    model.train()\n",
    "\n",
    "                if itr % SAVEPOINT_AFTER == 0:\n",
    "                    torch.save(model.state_dict(), self.model_fn)\n",
    "                    if verbose:\n",
    "                        print(f\"[Iter {itr}] Saved model at {self.model_fn}\")\n",
    "\n",
    "                itr += 1\n",
    "\n",
    "        # Save final model\n",
    "        torch.save(model.state_dict(), self.model_fn)\n",
    "        print(f\"Final model saved at {self.model_fn}\")\n",
    "        print(\"Done training.\")\n",
    "\n",
    "    def batch_constraint_violation_loss(self, dis_traj_pred, cont_traj_pred, lambda_penalty=1.0):\n",
    "        \"\"\"\n",
    "        Compute the constraint violation as loss function for a batch data\n",
    "        \"\"\"\n",
    "        batch_size = dis_traj_pred.size(0)\n",
    "        total_violation = 0.0\n",
    "        for b in range(batch_size):\n",
    "            dis_traj = NNoutput_reshape_torch(dis_traj_pred[b], self.n_obs)\n",
    "            cont_traj = cont_traj_pred[b]\n",
    "            total_violation += constraint_violation_torch(dis_traj, cont_traj)\n",
    "        return lambda_penalty * total_violation / batch_size\n",
    "\n",
    "    # Train with self-supervised loss function\n",
    "    def SS_train(self, training_params, verbose=True, penalty_weight = 1.0):\n",
    "        \"\"\"\n",
    "        Implement self-supervised learning with constraint violation based loss\n",
    "        penalty_weight: the penalty weight for constraint violation if linearly combine with supervised loss \n",
    "        \"\"\"\n",
    "        BATCH_SIZE = training_params['BATCH_SIZE']\n",
    "        TEST_BATCH_SIZE = training_params['TEST_BATCH_SIZE']\n",
    "        TRAINING_EPOCHS = training_params['TRAINING_EPOCHS']\n",
    "        CHECKPOINT_AFTER = training_params['CHECKPOINT_AFTER']\n",
    "        SAVEPOINT_AFTER = training_params['SAVEPOINT_AFTER']\n",
    "        LEARNING_RATE = training_params['LEARNING_RATE']\n",
    "        WEIGHT_DECAY = training_params['WEIGHT_DECAY']\n",
    "        EARLY_STOPPING_PATIENCE = training_params['EARLY_STOPPING_PATIENCE']\n",
    "\n",
    "        model = self.model\n",
    "        device = self.device\n",
    "        # Prepare dataset\n",
    "        X_tensor = torch.from_numpy(self.features).float()\n",
    "        Y_tensor = torch.from_numpy(self.outputs).float()\n",
    "        P_tensor = torch.from_numpy(self.P_train['XX'][:,:2,:]).float()\n",
    "        full_dataset = TensorDataset(X_tensor, Y_tensor, P_tensor)\n",
    "\n",
    "        # Split into train/val\n",
    "        num_total = len(full_dataset)\n",
    "        num_train = int(0.9*num_total)\n",
    "        train_dataset = TensorDataset(X_tensor[:num_train], Y_tensor[:num_train], P_tensor[:num_train])\n",
    "        val_dataset = TensorDataset(X_tensor[num_train:], Y_tensor[num_train:], P_tensor[num_train:])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # supervised loss and optimizer\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)        \n",
    "        best_val_loss = float('inf')\n",
    "        epochs_since_improvement = 0\n",
    "\n",
    "        itr = 1\n",
    "        for epoch in range(TRAINING_EPOCHS):\n",
    "            model.train()\n",
    "            for inputs, y_true, params in train_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                y_true = y_true.to(device)\n",
    "                params = params.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(inputs)  # shape: (B, N_obs*4*H)\n",
    "                loss = loss_fn(logits, y_true) + self.batch_constraint_violation_loss(logits, params, lambda_penalty=penalty_weight)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if itr % CHECKPOINT_AFTER == 0:\n",
    "                    # Evaluate on validation set\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        val_loss_total = 0\n",
    "                        val_cons_violation = []\n",
    "                        bitwise_accs = []\n",
    "\n",
    "                        for val_inputs, val_targets, val_params in val_loader:\n",
    "                            val_inputs = val_inputs.to(device)\n",
    "                            val_targets = val_targets.to(device)\n",
    "                            val_params = val_params.to(device)\n",
    "\n",
    "                            val_logits = model(val_inputs)\n",
    "                            val_loss = loss_fn(val_logits, val_targets) + self.batch_constraint_violation_loss(val_logits, val_params, lambda_penalty=penalty_weight)\n",
    "                            val_loss_total += val_loss.item()\n",
    "                            val_preds = val_logits.int() # Already rounded by STE_Round\n",
    "\n",
    "                            # Compare accuracy\n",
    "                            bitwise_accs.append(compute_bitwise_accuracy(val_preds, val_targets.int()))\n",
    "                            # Evaluate constraint violation\n",
    "                            constraint_loss = self.batch_constraint_violation_loss(val_preds, val_params).item()\n",
    "                            val_cons_violation.append(constraint_loss)\n",
    "\n",
    "                        avg_val_loss = val_loss_total/len(val_loader)\n",
    "                        avg_bitwise_acc = np.mean(bitwise_accs)\n",
    "                        avg_val_cons_violation = np.mean(val_cons_violation)\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f\"[Iter {itr}] Validation loss: {avg_val_loss:.4f} | \"\n",
    "                                f\"Validation accuracy (bitwise): {avg_bitwise_acc:.4f} | \"\n",
    "                                f\"Constraint violation: {avg_val_cons_violation:.4f}\")\n",
    "\n",
    "                        # Check for early stopping\n",
    "                        if avg_val_loss < best_val_loss - 1e-3:\n",
    "                            best_val_loss = avg_val_loss\n",
    "                            epochs_since_improvement = 0\n",
    "                        else:\n",
    "                            epochs_since_improvement += 1\n",
    "                            if epochs_since_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "                                print(f\"Early stopping: no improvement for {EARLY_STOPPING_PATIENCE} epoches\")\n",
    "                                torch.save(model.state_dict(), self.model_fn)\n",
    "                                print(f\"Final model saved at {self.model_fn}\")\n",
    "                                return  # Exit training early\n",
    "\n",
    "                    model.train()\n",
    "\n",
    "                if itr % SAVEPOINT_AFTER == 0:\n",
    "                    torch.save(model.state_dict(), self.model_fn)\n",
    "                    if verbose:\n",
    "                        print(f\"[Iter {itr}] Saved model at {self.model_fn}\")\n",
    "\n",
    "                itr += 1\n",
    "\n",
    "        # Save final model\n",
    "        torch.save(model.state_dict(), self.model_fn)\n",
    "        print(f\"Final model saved at {self.model_fn}\")\n",
    "        print(\"Done training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = os.getcwd()\n",
    "relative_path = os.path.abspath(\"..\")\n",
    "dataset_fn = relative_path + '/data' + '/single.p'\n",
    "prob_features = ['x0', 'xg']\n",
    "\n",
    "data_file = open(dataset_fn,'rb')\n",
    "all_data = pickle.load(data_file)[:10000]\n",
    "data_file.close()\n",
    "num_train = len(all_data)\n",
    "\n",
    "X0 = np.vstack([all_data[ii]['x0'].T for ii in range(num_train)])  \n",
    "XG = np.vstack([all_data[ii]['xg'].T for ii in range(num_train)])  \n",
    "OBS = np.vstack([all_data[ii]['xg'].T for ii in range(num_train)])  \n",
    "XX = np.array([all_data[ii]['XX'] for ii in range(num_train)])\n",
    "UU = np.array([all_data[ii]['UU'] for ii in range(num_train)])\n",
    "YY = np.concatenate([all_data[ii]['YY'].astype(int) for ii in range(num_train)], axis=1).transpose(1,0,2)\n",
    "train_data = [{'x0': X0, 'xg': XG}, {'XX': XX, 'UU' : UU}, YY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Regression with Feed-forward NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the FFNet model\n",
    "FFNet_reg = Regression(prob_features)\n",
    "n_features = 6 # the dimension of feature (input vector)\n",
    "FFNet_reg.setup_data(n_features, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNet_reg.setup_network(depth=4, neurons=1024)\n",
    "FFNet_reg.model\n",
    "\n",
    "training_params = {}\n",
    "training_params['TRAINING_EPOCHS'] = int(2000)\n",
    "training_params['BATCH_SIZE'] = 200\n",
    "training_params['CHECKPOINT_AFTER'] = int(1e3)\n",
    "training_params['SAVEPOINT_AFTER'] = int(1e4)\n",
    "training_params['TEST_BATCH_SIZE'] = 100\n",
    "training_params['LEARNING_RATE'] = 1e-3\n",
    "training_params['WEIGHT_DECAY'] = 1e-4\n",
    "training_params['EARLY_STOPPING_PATIENCE'] = 10\n",
    "\n",
    "FFNet_reg.train(training_params, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Refine with Self-Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading presaved Hetero GNN classifier model from regression_20250718_1654.pt\n",
      "FFNet(\n",
      "  (activation): ReLU()\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=6, out_features=1024, bias=True)\n",
      "    (1-3): 3 x Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (4): Linear(in_features=1024, out_features=160, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "fn_saved = 'regression_20250718_1654.pt' \n",
    "FFNet_reg.setup_network(depth=4, neurons=1024)\n",
    "FFNet_reg.load_network(fn_saved)\n",
    "print(FFNet_reg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 100] Validation loss: 16.2816 | Validation accuracy (bitwise): 0.6893 | Constraint violation: 1.5725\n",
      "[Iter 200] Validation loss: 3.8188 | Validation accuracy (bitwise): 0.7488 | Constraint violation: 0.3285\n",
      "[Iter 300] Validation loss: 8.2950 | Validation accuracy (bitwise): 0.7219 | Constraint violation: 0.7751\n",
      "[Iter 400] Validation loss: 7.7886 | Validation accuracy (bitwise): 0.7165 | Constraint violation: 0.7242\n",
      "[Iter 500] Validation loss: 8.1592 | Validation accuracy (bitwise): 0.7159 | Constraint violation: 0.7613\n"
     ]
    }
   ],
   "source": [
    "training_params = {}\n",
    "training_params['TRAINING_EPOCHS'] = int(200)\n",
    "training_params['BATCH_SIZE'] = 200\n",
    "training_params['CHECKPOINT_AFTER'] = int(1e2)\n",
    "training_params['SAVEPOINT_AFTER'] = int(1e3)\n",
    "training_params['TEST_BATCH_SIZE'] = 100\n",
    "training_params['LEARNING_RATE'] = 1e-3\n",
    "training_params['WEIGHT_DECAY'] = 1e-4\n",
    "training_params['EARLY_STOPPING_PATIENCE'] = 10\n",
    "\n",
    "FFNet_reg.SS_train(training_params, verbose=True, penalty_weight=10.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
