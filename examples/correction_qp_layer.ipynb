{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758d92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as patheffects\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c907797",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "\n",
    "We solve the following MIQP problem:\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\text{minimize } && \\mathbf{x}^T \\mathbf{x} + \\mathbf{p}^T \\mathbf{x} \\\\\n",
    "&\\text{subject to} && \\mathbf{x} \\le \\mathbf{b}, \\\\\n",
    "& && \\mathbf{1}^T \\mathbf{x} \\le a, \\\\\n",
    "& && \\mathbf{x} \\le M \\mathbf{y}, \\\\\n",
    "& && \\mathbf{1}^T \\mathbf{y} \\le 1,\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^2$ and $\\mathbf{y} \\in \\{0,1\\}^2$ are the continuous and binary optimization variables. The parameters include $\\mathbf{p}$, $\\mathbf{b}$, and $a$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625c3c7",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d91a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "nx = 2  # number of continuous decision variables\n",
    "ny = 2  # number of integer decision variables\n",
    "data_seed = 18\n",
    "np.random.seed(data_seed)\n",
    "torch.manual_seed(data_seed)\n",
    "\n",
    "p_low, p_high = -30.0, 5.0   # linear term in objective\n",
    "b_low, b_high = 5.0, 25.0    # RHS of constraint x <= b\n",
    "a_low, a_high = 10.0, 30.0   # RHS of constraint 1^T x <= a\n",
    "\n",
    "ntrain = 50000\n",
    "ntest = 1000\n",
    "\n",
    "# Generate samples\n",
    "samples_train = {\n",
    "    \"p\": torch.FloatTensor(ntrain, nx).uniform_(p_low, p_high),\n",
    "    \"b\": torch.FloatTensor(ntrain, nx).uniform_(b_low, b_high),\n",
    "    \"a\": torch.FloatTensor(ntrain, 1).uniform_(a_low, a_high),\n",
    "}\n",
    "\n",
    "samples_dev = {\n",
    "    \"p\": torch.FloatTensor(ntrain, nx).uniform_(p_low, p_high),\n",
    "    \"b\": torch.FloatTensor(ntrain, nx).uniform_(b_low, b_high),\n",
    "    \"a\": torch.FloatTensor(ntrain, 1).uniform_(a_low, a_high),\n",
    "}\n",
    "\n",
    "samples_test = {\n",
    "    \"p\": torch.FloatTensor(ntest, nx).uniform_(p_low, p_high),\n",
    "    \"b\": torch.FloatTensor(ntest, nx).uniform_(b_low, b_high),\n",
    "    \"a\": torch.FloatTensor(ntest, 1).uniform_(a_low, a_high),\n",
    "}\n",
    "\n",
    "# --- Custom dataset class ---\n",
    "class SampleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples[\"p\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"p\": self.samples[\"p\"][idx],\n",
    "            \"b\": self.samples[\"b\"][idx],\n",
    "            \"a\": self.samples[\"a\"][idx],\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SampleDataset(samples_train)\n",
    "dev_dataset = SampleDataset(samples_dev)\n",
    "test_dataset = SampleDataset(samples_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09849b4e",
   "metadata": {},
   "source": [
    "# Slacked QP Formulation\n",
    "\n",
    "We solve the following MIQP problem:\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\text{minimize } && \\mathbf{x}^\\top \\mathbf{x} + \\mathbf{p}^\\top \\mathbf{x} + \\mathbf{s}^\\top \\mathbf{s} \\\\\n",
    "&\\text{subject to} && \\mathbf{x} \\le \\mathbf{b}, \\\\\n",
    "& && \\mathbf{1}^\\top \\mathbf{x} \\le a, \\\\\n",
    "& && \\mathbf{x} \\le M \\mathbf{y} + \\mathbf{s}, \\\\\n",
    "& && \\mathbf{1}^\\top \\mathbf{y} \\le 1, \\\\\n",
    "& && \\mathbf{s} \\ge 0,\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^2$ and $\\mathbf{y} \\in \\{0,1\\}^2$ are the continuous and binary optimization variables. The parameters include $\\mathbf{p}$, $\\mathbf{b}$, and $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df93e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _check_shapes(p, b, a, y):\n",
    "    \"\"\"    Quick assertion helper. Accepts batched tensors:\n",
    "    - p: (B,n), b: (B,n), a: (B,1) or (B,), M: (B,n,ny), y: (B,ny)\n",
    "    \"\"\"\n",
    "    B = p.shape[0]\n",
    "    n = p.shape[1]\n",
    "    ny = y.shape[1]\n",
    "    assert b.shape == (B, n)\n",
    "    assert y.shape == (B, ny)\n",
    "    assert a.shape in [(B,), (B, 1)]\n",
    "    return B, n, ny\n",
    "\n",
    "def solve_qp_with_slacks(layer: CvxpyLayer, p, b, a, y):\n",
    "    \"\"\"\n",
    "    Run the CVXPYLayer in batch. All inputs are torch tensors.\n",
    "    Returns x, s (each torch tensor with grad).\n",
    "    \"\"\"\n",
    "    _, _, _ = _check_shapes(p, b, a, y)\n",
    "    if a.ndim == 1:\n",
    "        a = a.unsqueeze(-1)\n",
    "\n",
    "    # Device management\n",
    "    device = p.device\n",
    "    layer = layer.to(device)\n",
    "    p = p.to(device)\n",
    "    b = b.to(device)\n",
    "    a = a.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    x_opt, s_opt = layer(p, b, a, y)\n",
    "    return x_opt, s_opt\n",
    "\n",
    "def QP_Layer(nx, ny, penalty=\"l1\", rho1=1.0, bigM = 1e3, **kwargs):\n",
    "    # Define CVXPY variables and parameters\n",
    "    x = cp.Variable((nx,))  # continuous decision variables\n",
    "    y = cp.Parameter((ny,))  # integer decision variables\n",
    "\n",
    "    p = cp.Parameter((nx,))  # linear term in the objective\n",
    "    b = cp.Parameter((nx,))  # RHS of the constraint x <= b\n",
    "    a = cp.Parameter((1,))   # RHS of the constraint 1.T*x <= a\n",
    "    s = cp.Variable((nx,), nonneg=True)  # slack variables\n",
    "    \n",
    "    # Define the QP problem\n",
    "    if penalty == \"l1\": # default to l1 penalty\n",
    "        objective = cp.Minimize(cp.quad_form(x, np.eye(nx)) + p.T @ x + rho1*cp.sum(s))\n",
    "    elif penalty == \"l2\": \n",
    "        objective = cp.Minimize(cp.quad_form(x, np.eye(nx)) + p.T @ x + rho1*cp.quad_form(s, np.eye(nx)))\n",
    "    constraints = [\n",
    "        x <= b,\n",
    "        sum(x) <= a,\n",
    "        x <= bigM * y + s,\n",
    "        s >= 0,\n",
    "    ]\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    # Create CVXPY layer\n",
    "    cvxpylayer = CvxpyLayer(problem, parameters=[p, b, a, y], variables=[x, s])\n",
    "    return cvxpylayer\n",
    "\n",
    "def _solve_single_miqp(args):\n",
    "    \"\"\"Helper function to solve a single MIQP problem.\"\"\"\n",
    "    # Redirect stdout and stderr to devnull at the start of each process\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    sys.stderr = open(os.devnull, 'w')\n",
    "    \"\"\"Helper function to solve a single MIQP problem.\"\"\"\n",
    "    i, p_i, b_i, a_i, nx, ny = args\n",
    "    \n",
    "    # Variables\n",
    "    x = cp.Variable(nx)\n",
    "    y = cp.Variable(ny, boolean=True)\n",
    "    \n",
    "    # Objective and constraints\n",
    "    objective = cp.Minimize(cp.sum_squares(x) + p_i @ x)\n",
    "    constraints = [\n",
    "        x <= b_i,\n",
    "        cp.sum(x) <= a_i,\n",
    "        cp.sum(y) <= 1,\n",
    "        x <= 1e3 * y\n",
    "    ]\n",
    "    \n",
    "    # Problem definition\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    \n",
    "    try:\n",
    "        prob.solve(solver=cp.GUROBI, verbose=False, OutputFlag=0)\n",
    "        \n",
    "        if x.value is not None and y.value is not None:\n",
    "            return i, x.value, y.value\n",
    "        else:\n",
    "            return i, np.zeros(nx), np.zeros(ny)\n",
    "    except Exception as e:\n",
    "        print(f\"Error solving sample {i}: {e}\")\n",
    "        return i, np.zeros(nx), np.zeros(ny)\n",
    "\n",
    "@torch.no_grad()\n",
    "def GUROBI_solve_parallel(p: torch.Tensor, b: torch.Tensor, a: torch.Tensor, max_workers=None):\n",
    "    \"\"\"\n",
    "    Solve MIQP for each sample in the batch using parallel processing.\n",
    "    \"\"\"\n",
    "    device = p.device\n",
    "    p_np = p.detach().cpu().numpy()\n",
    "    b_np = b.detach().cpu().numpy()\n",
    "    a_np = a.detach().cpu().numpy()\n",
    "    \n",
    "    if a_np.ndim == 2:\n",
    "        a_np = a_np.squeeze(-1)\n",
    "    \n",
    "    B = p_np.shape[0]\n",
    "    \n",
    "    # Prepare arguments for parallel execution\n",
    "    args_list = [(i, p_np[i], b_np[i], a_np[i], nx, ny) for i in range(B)]\n",
    "    \n",
    "    # Preallocate result arrays\n",
    "    x_results = np.zeros((B, nx))\n",
    "    y_results = np.zeros((B, ny))\n",
    "    \n",
    "    # Use ProcessPoolExecutor for parallel solving\n",
    "    if max_workers is None:\n",
    "        max_workers = min(B, mp.cpu_count())\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(_solve_single_miqp, args): args[0] for args in args_list}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            i, x_sol, y_sol = future.result()\n",
    "            x_results[i] = x_sol\n",
    "            y_results[i] = y_sol\n",
    "    \n",
    "    return torch.tensor(x_results).float().to(device), torch.tensor(y_results).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdefb2e3",
   "metadata": {},
   "source": [
    "### Supervised learning demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfcc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SL(nn_model, train_loader, test_loader, training_params, device = None, wandb_log = False):\n",
    "    TRAINING_EPOCHS = training_params['TRAINING_EPOCHS']\n",
    "    CHECKPOINT_AFTER = training_params['CHECKPOINT_AFTER']\n",
    "    LEARNING_RATE = training_params['LEARNING_RATE']\n",
    "    WEIGHT_DECAY = training_params['WEIGHT_DECAY']\n",
    "    PATIENCE = training_params['PATIENCE']\n",
    "    # Put all layers in device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    nn_model.to(device)\n",
    "\n",
    "    global_step = 0\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=int(PATIENCE/2))\n",
    "    supervised_loss_fn = nn.HuberLoss() \n",
    "    best_val_loss = float(\"inf\") # Store best validation \n",
    "    epochs_no_improve = 0  # Count epochs with no improvement  \n",
    "    if wandb_log: wandb.init(\n",
    "        project=training_params.get(\"WANDB_PROJECT\", \"supervised_learning\"),\n",
    "        name=training_params.get(\"RUN_NAME\", None),\n",
    "        config=training_params\n",
    "    )    \n",
    "\n",
    "    for epoch in range(1, TRAINING_EPOCHS+1):\n",
    "        nn_model.train()\n",
    "        for batch in train_loader:\n",
    "            p_batch = batch['p'].to(device); b_batch = batch['b'].to(device); a_batch = batch['a'].to(device)\n",
    "            B = p_batch.shape[0]\n",
    "            # ---- Predict y from theta = [p,b,a] ----\n",
    "            theta = torch.cat([p_batch, b_batch, a_batch], dim=-1)  # (B, 2n+1)\n",
    "            y_pred = nn_model(theta).float() # (B, ny), hard {0,1}\n",
    "            # May need it to include a supervised loss function \n",
    "            x_solver, y_solver = GUROBI_solve_parallel(p_batch, b_batch, a_batch)\n",
    "            supervised_loss = supervised_loss_fn(y_pred, y_solver.float())\n",
    "            loss = supervised_loss\n",
    "            if wandb_log: wandb.log({\n",
    "                \"train/loss\": loss.item(),\n",
    "                \"step\": global_step})\n",
    "            # ---- Backprop ----\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(nn_model.parameters(), max_norm=1e1)\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            \n",
    "            # ---- Logging ----\n",
    "            if global_step == 1 or (global_step % CHECKPOINT_AFTER) == 0:\n",
    "                training_loss = loss.item()\n",
    "                val_loss_total = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for val_batch in test_loader:\n",
    "                        p_batch = val_batch['p'].to(device); b_batch = val_batch['b'].to(device); a_batch = val_batch['a'].to(device)\n",
    "                        # ---- Predict y from theta = [p,b,a] ----\n",
    "                        theta = torch.cat([p_batch, b_batch, a_batch], dim=-1)  # (B, 2n+1)\n",
    "                        y_pred_test = nn_model(theta).float() # (B, ny), hard {0,1}\n",
    "                        _, y_solver_test = GUROBI_solve_parallel(p_batch, b_batch, a_batch)\n",
    "                        val_loss_total += supervised_loss_fn(y_pred_test, y_solver_test).item()                           \n",
    "                    avg_val_loss = val_loss_total / len(test_loader)\n",
    "\n",
    "                print(f\"[epoch {epoch} | step {global_step}] \"\n",
    "                    f\"training loss = {training_loss:.4f}, \"\n",
    "                    f\"validation loss = {avg_val_loss:.4f}\")\n",
    "                # --- Log losses to wandb ---\n",
    "                if wandb_log: wandb.log({\n",
    "                    \"val/loss\": avg_val_loss,\n",
    "                    \"epoch\": epoch})\n",
    "\n",
    "                # Check if need to update the learning rates\n",
    "                last_lr = optimizer.param_groups[0]['lr']\n",
    "                scheduler.step(loss.item())\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                if current_lr != last_lr:\n",
    "                    print(f\"Learning rate updated: {last_lr:.6f} -> {current_lr:.6f}\")\n",
    "                    last_lr = current_lr\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve >= PATIENCE:\n",
    "                        print(\"Early stopping triggered!\")\n",
    "                        return\n",
    "    if wandb_log: wandb.finish()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f563e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 500\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6778c812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1 | step 1] training loss = 0.2290, validation loss = 0.2260\n",
      "[epoch 1 | step 20] training loss = 0.0340, validation loss = 0.0220\n",
      "[epoch 1 | step 40] training loss = 0.0195, validation loss = 0.0130\n",
      "[epoch 1 | step 60] training loss = 0.0155, validation loss = 0.0108\n",
      "[epoch 1 | step 80] training loss = 0.0090, validation loss = 0.0095\n",
      "[epoch 1 | step 100] training loss = 0.0060, validation loss = 0.0088\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SL_model = MLPWithSTE(insize=2*nx+1, outsize=ny,\n",
    "                bias=True,\n",
    "                linear_map=torch.nn.Linear,\n",
    "                nonlin=nn.ReLU,\n",
    "                hsizes=[128] * 2)\n",
    "\n",
    "training_params = {}\n",
    "training_params['TRAINING_EPOCHS'] = int(1)\n",
    "training_params['CHECKPOINT_AFTER'] = int(20)\n",
    "training_params['LEARNING_RATE'] = 1e-3\n",
    "training_params['WEIGHT_DECAY'] = 1e-5\n",
    "training_params['PATIENCE'] = 5\n",
    "\n",
    "train_SL(SL_model, train_loader, test_loader, training_params, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69847c5",
   "metadata": {},
   "source": [
    "### Self-Supervised Learning Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82fa707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some penalty functions that we may use\n",
    "l1_penalty = lambda s: s.sum(dim=1)\n",
    "l2_penalty = lambda s: (s**2).sum(dim=1)\n",
    "\n",
    "def combined_loss_fcn(loss_components, weights):\n",
    "    \"\"\"\n",
    "    Combine multiple loss components with given weights.\n",
    "    \"\"\"\n",
    "    assert len(loss_components) == len(weights), \"Number of loss components must match number of weights.\"\n",
    "    combined_loss = sum(w * lc for w, lc in zip(weights, loss_components))\n",
    "    return combined_loss\n",
    "\n",
    "def train_SSL(nn_model, cvx_layer, sl_model, train_loader, test_loader, training_params,  loss_weights, \n",
    "            slack_penalty = l1_penalty, constraint_penalty = torch.relu,\n",
    "            device = None, wandb_log = False):\n",
    "    TRAINING_EPOCHS = training_params['TRAINING_EPOCHS']\n",
    "    CHECKPOINT_AFTER = training_params['CHECKPOINT_AFTER']\n",
    "    LEARNING_RATE = training_params['LEARNING_RATE']\n",
    "    WEIGHT_DECAY = training_params['WEIGHT_DECAY']\n",
    "    PATIENCE = training_params['PATIENCE']\n",
    "    # Put all layers in device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    nn_model.to(device)\n",
    "    nn_model.train()\n",
    "    cvx_layer.to(device)\n",
    "    sl_model.to(device)\n",
    "    sl_model.eval()\n",
    "\n",
    "    # Define weights for loss components\n",
    "    weights = torch.tensor(loss_weights, device=device)\n",
    "    weights = weights / weights.sum()\n",
    "    global_step = 0\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=int(PATIENCE/2))\n",
    "    supervised_loss_fn = nn.HuberLoss() \n",
    "    best_val_loss = float(\"inf\") # Store best validation \n",
    "    epochs_no_improve = 0  # Count epochs with no improvement \n",
    "\n",
    "    if wandb_log: wandb.init(\n",
    "        project=training_params.get(\"WANDB_PROJECT\", \"self_supervised_learning\"),\n",
    "        name=training_params.get(\"RUN_NAME\", None),\n",
    "        config=training_params\n",
    "    )       \n",
    "\n",
    "    # Validation for the supervised learning model\n",
    "    print(\"Validation for the supervised learning model: \")\n",
    "    val_loss_total = []; obj_val_total = []; slack_pen_total = []; y_sum_penalty_total = []; supervised_loss_total = []\n",
    "    with torch.no_grad():\n",
    "        for val_batch in test_loader:\n",
    "            p_batch = val_batch['p'].to(device); b_batch = val_batch['b'].to(device); a_batch = val_batch['a'].to(device)\n",
    "            # ---- Predict y from theta = [p,b,a] ----\n",
    "            theta = torch.cat([p_batch, b_batch, a_batch], dim=-1)  # (B, 2n+1)\n",
    "            # prediction from the supervised learning model\n",
    "            y_pred_hat = sl_model(theta).float() # (B, ny), hard {0,1}\n",
    "            # ---- Solve convex subproblem given y ----\n",
    "            # CVXPYLayer supports autograd; keep inputs requiring grad if needed\n",
    "            x_opt, s_opt = solve_qp_with_slacks(cvx_layer, p_batch, b_batch, a_batch, y_pred_hat)\n",
    "            # May need it to include a supervised loss function \n",
    "            x_solver, y_solver = GUROBI_solve_parallel(p_batch, b_batch, a_batch)\n",
    "            supervised_loss = supervised_loss_fn(y_pred_hat, y_solver.float())\n",
    "            obj_val = ((x_opt**2).sum(dim=1) + (p_batch * x_opt).sum(dim=1)).mean()  # (B,)\n",
    "            # Slack penalty, for constraint violation of the continuous decision variables\n",
    "            slack_pen = slack_penalty(s_opt).mean()\n",
    "            # Violation penalty for constraint violation with the integer decision variables\n",
    "            y_sum_penalty = constraint_penalty(y_pred_hat.sum(dim=1) - 1.0).mean() # (B,)\n",
    "            # Collect loss values\n",
    "            obj_val_total.append(obj_val.item())\n",
    "            slack_pen_total.append(slack_pen.item())\n",
    "            y_sum_penalty_total.append(y_sum_penalty.item())\n",
    "            supervised_loss_total.append(supervised_loss.item())\n",
    "\n",
    "        # Compute the averages\n",
    "        avg_obj_val = torch.mean(torch.tensor(obj_val_total))\n",
    "        avg_slack_pen = torch.mean(torch.tensor(slack_pen_total))\n",
    "        avg_y_sum_penalty = torch.mean(torch.tensor(y_sum_penalty_total))\n",
    "        avg_supervised_loss = torch.mean(torch.tensor(supervised_loss_total))\n",
    "\n",
    "        print(\n",
    "            f\"supervised learning model: \"\n",
    "            f\"obj_val = {avg_obj_val:.4f}, \"\n",
    "            f\"slack_pen = {avg_slack_pen:.4f}, \"\n",
    "            f\"y_sum_penalty = {avg_y_sum_penalty:.4f}, \"\n",
    "            f\"supervised_loss = {avg_supervised_loss:.4f}, \")    \n",
    "    print(\"_\"*50)\n",
    "\n",
    "    for epoch in range(1, TRAINING_EPOCHS+1):\n",
    "        nn_model.train()\n",
    "        for batch in train_loader:\n",
    "            p_batch = batch['p'].to(device); b_batch = batch['b'].to(device); a_batch = batch['a'].to(device)\n",
    "            B = p_batch.shape[0]\n",
    "            # ---- Predict y from theta = [p,b,a] ----\n",
    "            theta = torch.cat([p_batch, b_batch, a_batch], dim=-1)  # (B, 2n+1)\n",
    "            # prediction from the supervised learning model\n",
    "            y_pred_hat = sl_model(theta).float() # (B, ny), hard {0,1}\n",
    "            # construct the concatenated input\n",
    "            concat_input = torch.cat([theta, y_pred_hat], dim=-1)\n",
    "            y_pred = nn_model(concat_input).float() # (B, ny), hard {0,1}\n",
    "            # ---- Solve convex subproblem given y ----\n",
    "            # CVXPYLayer supports autograd; keep inputs requiring grad if needed\n",
    "            x_opt, s_opt = solve_qp_with_slacks(cvx_layer, p_batch, b_batch, a_batch, y_pred)\n",
    "            obj_val = ((x_opt**2).sum(dim=1) + (p_batch * x_opt).sum(dim=1)).mean()  # (B,)\n",
    "            # Slack penalty, for constraint violation of the continuous decision variables\n",
    "            slack_pen = slack_penalty(s_opt).mean()\n",
    "            # Violation penalty for constraint violation with the integer decision variables\n",
    "            y_sum_penalty = constraint_penalty(y_pred.sum(dim=1) - 1.0).mean()  # (B,)\n",
    "            # supervised learning loss\n",
    "            deviation_loss = supervised_loss_fn(y_pred, y_pred_hat.float())\n",
    "            # Total loss with balanced weights\n",
    "            loss = combined_loss_fcn([obj_val, slack_pen, y_sum_penalty, deviation_loss], weights)\n",
    "            if wandb_log: wandb.log({\n",
    "                \"train/combined_loss\": loss.item(),\n",
    "                \"train/obj_val\": obj_val.item(),\n",
    "                \"train/slack_pen\": slack_pen.item(),\n",
    "                \"train/y_sum_penalty\": y_sum_penalty.item(),\n",
    "                \"train/deviation_loss\": deviation_loss.item(),\n",
    "                \"step\": global_step})\n",
    "\n",
    "            # ---- Backprop ----\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(nn_model.parameters(), max_norm=1e1)\n",
    "            optimizer.step()         \n",
    "            global_step += 1\n",
    "\n",
    "            # ---- Logging ----\n",
    "            if global_step == 1 or (global_step % CHECKPOINT_AFTER) == 0:\n",
    "                val_loss_total = []; obj_val_total = []; slack_pen_total = []; y_sum_penalty_total = []; supervised_loss_total = []\n",
    "                with torch.no_grad():\n",
    "                    for val_batch in test_loader:\n",
    "                        p_batch = val_batch['p'].to(device); b_batch = val_batch['b'].to(device); a_batch = val_batch['a'].to(device)\n",
    "                        # ---- Predict y from theta = [p,b,a] ----\n",
    "                        theta = torch.cat([p_batch, b_batch, a_batch], dim=-1)  # (B, 2n+1)\n",
    "                        # prediction from the supervised learning model\n",
    "                        y_pred_hat = sl_model(theta).float() # (B, ny), hard {0,1}\n",
    "                        # construct the concatenated input\n",
    "                        concat_input = torch.cat([theta, y_pred_hat], dim=-1)\n",
    "                        y_pred_test = nn_model(concat_input).float() # (B, ny), hard {0,1}\n",
    "                        # ---- Solve convex subproblem given y ----\n",
    "                        # CVXPYLayer supports autograd; keep inputs requiring grad if needed\n",
    "                        x_opt, s_opt = solve_qp_with_slacks(cvx_layer, p_batch, b_batch, a_batch, y_pred_test)\n",
    "                        # May need it to include a supervised loss function \n",
    "                        x_solver, y_solver = GUROBI_solve_parallel(p_batch, b_batch, a_batch)\n",
    "                        supervised_loss = supervised_loss_fn(y_pred_test, y_solver.float())\n",
    "                        obj_val = ((x_opt**2).sum(dim=1) + (p_batch * x_opt).sum(dim=1)).mean()  # (B,)\n",
    "                        # Slack penalty, for constraint violation of the continuous decision variables\n",
    "                        slack_pen = slack_penalty(s_opt).mean()\n",
    "                        # Violation penalty for constraint violation with the integer decision variables\n",
    "                        y_sum_penalty = constraint_penalty(y_pred_test.sum(dim=1) - 1.0).mean() # (B,)\n",
    "                        loss = combined_loss_fcn([obj_val, slack_pen, y_sum_penalty, supervised_loss], weights)\n",
    "                        # Collect loss values\n",
    "                        val_loss_total.append(loss.item())       \n",
    "                        obj_val_total.append(obj_val.item())\n",
    "                        slack_pen_total.append(slack_pen.item())\n",
    "                        y_sum_penalty_total.append(y_sum_penalty.item())\n",
    "                        supervised_loss_total.append(supervised_loss.item())\n",
    "\n",
    "                    # Compute the averages\n",
    "                    avg_val_loss = torch.mean(torch.tensor(val_loss_total))\n",
    "                    avg_obj_val = torch.mean(torch.tensor(obj_val_total))\n",
    "                    avg_slack_pen = torch.mean(torch.tensor(slack_pen_total))\n",
    "                    avg_y_sum_penalty = torch.mean(torch.tensor(y_sum_penalty_total))\n",
    "                    avg_supervised_loss = torch.mean(torch.tensor(supervised_loss_total))\n",
    "\n",
    "                    print(f\"[epoch {epoch} | step {global_step}] \"\n",
    "                        f\"validation: loss = {avg_val_loss:.4f}, \"\n",
    "                        f\"obj_val = {avg_obj_val:.4f}, \"\n",
    "                        f\"slack_pen = {avg_slack_pen:.4f}, \"\n",
    "                        f\"y_sum_penalty = {avg_y_sum_penalty:.4f}, \"\n",
    "                        f\"supervised_loss = {avg_supervised_loss:.4f}, \")\n",
    "                    # --- Log losses to wandb ---\n",
    "                    if wandb_log: wandb.log({\n",
    "                        \"val/avg_loss\": avg_val_loss,\n",
    "                        \"val/obj_val\": obj_val,\n",
    "                        \"val/slack_pen\": slack_pen,\n",
    "                        \"val/y_sum_penalty\": y_sum_penalty,\n",
    "                        \"val/supervised_loss\": supervised_loss,\n",
    "                        \"epoch\": epoch})                \n",
    "\n",
    "                # Check if need to update the learning rates\n",
    "                last_lr = optimizer.param_groups[0]['lr']\n",
    "                scheduler.step(loss.item())\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                if current_lr != last_lr:\n",
    "                    print(f\"Learning rate updated: {last_lr:.6f} -> {current_lr:.6f}\")\n",
    "                    last_lr = current_lr\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve >= PATIENCE:\n",
    "                        print(\"Early stopping triggered!\")\n",
    "                        return          \n",
    "\n",
    "    if wandb_log: wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5c28b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation for the supervised learning model: \n",
      "supervised learning model: obj_val = -97.7022, slack_pen = 0.0000, y_sum_penalty = 0.0030, supervised_loss = 0.0088, \n",
      "__________________________________________________\n",
      "[epoch 1 | step 1] validation: loss = 0.0236, obj_val = -63.3737, slack_pen = 0.0000, y_sum_penalty = 0.0020, supervised_loss = 0.2630, \n",
      "[epoch 1 | step 20] validation: loss = 0.0230, obj_val = -21.0139, slack_pen = -0.0000, y_sum_penalty = 0.0030, supervised_loss = 0.2455, \n",
      "[epoch 1 | step 40] validation: loss = 0.0105, obj_val = -63.2487, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.1265, \n",
      "[epoch 1 | step 60] validation: loss = 0.0103, obj_val = -89.4927, slack_pen = 0.0000, y_sum_penalty = 0.0060, supervised_loss = 0.0640, \n",
      "[epoch 1 | step 80] validation: loss = 0.0044, obj_val = -91.4979, slack_pen = 0.0000, y_sum_penalty = 0.0010, supervised_loss = 0.0430, \n",
      "[epoch 1 | step 100] validation: loss = 0.0027, obj_val = -92.1142, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0323, \n",
      "[epoch 2 | step 120] validation: loss = 0.0035, obj_val = -92.2693, slack_pen = 0.0000, y_sum_penalty = 0.0010, supervised_loss = 0.0320, \n",
      "[epoch 2 | step 140] validation: loss = 0.0045, obj_val = -94.2973, slack_pen = 0.0000, y_sum_penalty = 0.0030, supervised_loss = 0.0243, \n",
      "[epoch 2 | step 160] validation: loss = 0.0036, obj_val = -94.8423, slack_pen = 0.0000, y_sum_penalty = 0.0020, supervised_loss = 0.0235, \n",
      "[epoch 2 | step 180] validation: loss = 0.0033, obj_val = -95.2975, slack_pen = 0.0000, y_sum_penalty = 0.0020, supervised_loss = 0.0200, \n",
      "[epoch 2 | step 200] validation: loss = 0.0022, obj_val = -92.0721, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0260, \n",
      "[epoch 3 | step 220] validation: loss = 0.0029, obj_val = -87.2275, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0343, \n",
      "[epoch 3 | step 240] validation: loss = 0.0018, obj_val = -93.4882, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0210, \n",
      "[epoch 3 | step 260] validation: loss = 0.0030, obj_val = -95.5864, slack_pen = 0.0000, y_sum_penalty = 0.0020, supervised_loss = 0.0158, \n",
      "[epoch 3 | step 280] validation: loss = 0.0015, obj_val = -94.6052, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0185, \n",
      "[epoch 3 | step 300] validation: loss = 0.0019, obj_val = -92.6789, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0223, \n",
      "[epoch 4 | step 320] validation: loss = 0.0036, obj_val = -97.0520, slack_pen = 0.0000, y_sum_penalty = 0.0030, supervised_loss = 0.0130, \n",
      "[epoch 4 | step 340] validation: loss = 0.0019, obj_val = -96.4805, slack_pen = 0.0000, y_sum_penalty = 0.0010, supervised_loss = 0.0130, \n",
      "[epoch 4 | step 360] validation: loss = 0.0012, obj_val = -95.1957, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0143, \n",
      "[epoch 4 | step 380] validation: loss = 0.0010, obj_val = -96.4925, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0125, \n",
      "[epoch 4 | step 400] validation: loss = 0.0012, obj_val = -95.4157, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0140, \n",
      "[epoch 5 | step 420] validation: loss = 0.0011, obj_val = -95.8739, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0130, \n",
      "[epoch 5 | step 440] validation: loss = 0.0014, obj_val = -93.3896, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0168, \n",
      "[epoch 5 | step 460] validation: loss = 0.0012, obj_val = -95.0237, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0140, \n",
      "[epoch 5 | step 480] validation: loss = 0.0012, obj_val = -95.3689, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0145, \n",
      "[epoch 5 | step 500] validation: loss = 0.0011, obj_val = -95.4991, slack_pen = 0.0000, y_sum_penalty = 0.0000, supervised_loss = 0.0137, \n",
      "Learning rate updated: 0.000100 -> 0.000050\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    device\n",
    "    SSL_model\n",
    "except NameError:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    SSL_model = MLPWithSTE(insize=2*nx+1+ny, outsize=ny,\n",
    "                bias=True,\n",
    "                linear_map=torch.nn.Linear,\n",
    "                nonlin=nn.ReLU,\n",
    "                hsizes=[128] * 2)\n",
    "\n",
    "# Then refine with with self-supervised learning\n",
    "training_params = {}\n",
    "training_params['TRAINING_EPOCHS'] = int(5)\n",
    "training_params['CHECKPOINT_AFTER'] = int(20)\n",
    "training_params['LEARNING_RATE'] = 1e-4\n",
    "training_params['WEIGHT_DECAY'] = 1e-5\n",
    "training_params['PATIENCE'] = 10\n",
    "\n",
    "slack_weight = 1e3\n",
    "constraint_weight = 1e4\n",
    "supervised_weight = 1e3\n",
    "cvx_layer = QP_Layer(nx=nx, ny=ny, penalty=\"l1\", rho1=slack_weight)\n",
    "loss_weights = [0.0, slack_weight, constraint_weight, supervised_weight]\n",
    "train_SSL(SSL_model, cvx_layer, SL_model, train_loader, test_loader, training_params, loss_weights,\n",
    "    device = device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c46d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f494ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2ee8a7f",
   "metadata": {},
   "source": [
    "### Batch Solve Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "p_batch = batch['p']\n",
    "b_batch = batch['b']\n",
    "a_batch = batch['a']\n",
    "B = p_batch.shape[0]\n",
    "M_batch = torch.ones(B, nx, ny) * 50.0  # Big-M matrix\n",
    "\n",
    "device = p_batch.device  # use your current device\n",
    "B, n = p_batch.shape\n",
    "ny = M_batch.shape[-1]\n",
    "\n",
    "# 4.1 Build the layer once\n",
    "cvx_layer = QP_Layer(nx=n, ny=ny, penalty=\"l1\", rho1=1e1)\n",
    "\n",
    "nn_model = MLPWithSTE(insize=2*nx+1, outsize=ny,\n",
    "            bias=True,\n",
    "            linear_map=torch.nn.Linear,\n",
    "            nonlin=nn.ReLU,\n",
    "            hsizes=[128] * 4)\n",
    "\n",
    "# 4.2 Get y from your classifier\n",
    "# Concatenate inputs\n",
    "theta = torch.cat([p_batch, b_batch, a_batch], dim=1)  # shape: (B, nx + ny + 1)\n",
    "# Forward pass through classifier\n",
    "y_pred_hard = nn_model(theta).float()  # shape: (B, ny)\n",
    "y_pred_int = y_pred_hard.int()\n",
    "\n",
    "# 4.3 Solve the convex subproblem given y\n",
    "x_opt, s_opt = solve_qp_with_slacks(\n",
    "    cvx_layer, p_batch, b_batch, a_batch, y_pred_hard\n",
    ")\n",
    "obj_val = torch.sum(x_opt**2, dim=1) + torch.sum(p_batch * x_opt, dim=1)  # (B,)\n",
    "\n",
    "# 4.4 Solve with GUROBI for comparison\n",
    "x_solver, y_solver = GUROBI_solve_parallel(p_batch, b_batch, a_batch)\n",
    "\n",
    "if True:\n",
    "    # print(\"NN Solution x:\", x_opt)\n",
    "    # print(\"GUROBI Solution x:\", x_solver)\n",
    "    print(\"Objective values:\", obj_val)\n",
    "    print(\"Slack:\", s_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1f5be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penn_env312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
