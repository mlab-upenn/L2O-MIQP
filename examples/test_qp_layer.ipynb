{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758d92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as patheffects\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c907797",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "\n",
    "We solve the following MIQP problem:\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\text{minimize } && \\mathbf{x}^T \\mathbf{x} + \\mathbf{p}^T \\mathbf{x} \\\\\n",
    "&\\text{subject to} && \\mathbf{x} \\le \\mathbf{b}, \\\\\n",
    "& && \\mathbf{1}^T \\mathbf{x} \\le a, \\\\\n",
    "& && \\mathbf{x} \\le M \\mathbf{y}, \\\\\n",
    "& && \\mathbf{1}^T \\mathbf{y} \\le 1,\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^2$ and $\\mathbf{y} \\in \\{0,1\\}^2$ are the continuous and binary optimization variables. The parameters include $\\mathbf{p}$, $\\mathbf{b}$, and $a$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625c3c7",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d91a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "nx = 2  # number of continuous decision variables\n",
    "ny = 2  # number of integer decision variables\n",
    "data_seed = 18\n",
    "np.random.seed(data_seed)\n",
    "torch.manual_seed(data_seed)\n",
    "\n",
    "p_low, p_high = -30.0, 5.0   # linear term in objective\n",
    "b_low, b_high = 5.0, 25.0    # RHS of constraint x <= b\n",
    "a_low, a_high = 10.0, 30.0   # RHS of constraint 1^T x <= a\n",
    "\n",
    "ntrain = 10000\n",
    "ntest = 500\n",
    "\n",
    "# Generate samples\n",
    "samples_train = {\n",
    "    \"p\": torch.FloatTensor(ntrain, nx).uniform_(p_low, p_high),\n",
    "    \"b\": torch.FloatTensor(ntrain, nx).uniform_(b_low, b_high),\n",
    "    \"a\": torch.FloatTensor(ntrain, 1).uniform_(a_low, a_high),\n",
    "}\n",
    "\n",
    "samples_dev = {\n",
    "    \"p\": torch.FloatTensor(ntrain, nx).uniform_(p_low, p_high),\n",
    "    \"b\": torch.FloatTensor(ntrain, nx).uniform_(b_low, b_high),\n",
    "    \"a\": torch.FloatTensor(ntrain, 1).uniform_(a_low, a_high),\n",
    "}\n",
    "\n",
    "samples_test = {\n",
    "    \"p\": torch.FloatTensor(ntest, nx).uniform_(p_low, p_high),\n",
    "    \"b\": torch.FloatTensor(ntest, nx).uniform_(b_low, b_high),\n",
    "    \"a\": torch.FloatTensor(ntest, 1).uniform_(a_low, a_high),\n",
    "}\n",
    "\n",
    "# --- Custom dataset class ---\n",
    "class SampleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples[\"p\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"p\": self.samples[\"p\"][idx],\n",
    "            \"b\": self.samples[\"b\"][idx],\n",
    "            \"a\": self.samples[\"a\"][idx],\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SampleDataset(samples_train)\n",
    "dev_dataset = SampleDataset(samples_dev)\n",
    "test_dataset = SampleDataset(samples_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09849b4e",
   "metadata": {},
   "source": [
    "# Slacked QP Formulation\n",
    "\n",
    "We solve the following MIQP problem:\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\text{minimize } && \\mathbf{x}^\\top \\mathbf{x} + \\mathbf{p}^\\top \\mathbf{x} + \\mathbf{s}^\\top \\mathbf{s} \\\\\n",
    "&\\text{subject to} && \\mathbf{x} \\le \\mathbf{b}, \\\\\n",
    "& && \\mathbf{1}^\\top \\mathbf{x} \\le a, \\\\\n",
    "& && \\mathbf{x} \\le M \\mathbf{y} + \\mathbf{s}, \\\\\n",
    "& && \\mathbf{1}^\\top \\mathbf{y} \\le 1, \\\\\n",
    "& && \\mathbf{s} \\ge 0,\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^2$ and $\\mathbf{y} \\in \\{0,1\\}^2$ are the continuous and binary optimization variables. The parameters include $\\mathbf{p}$, $\\mathbf{b}$, and $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79d666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QP_Layer(nx, ny, penalty=\"l1\", rho1=1.0, bigM = 1e3, **kwargs):\n",
    "    # Define CVXPY variables and parameters\n",
    "    x = cp.Variable((nx,))  # continuous decision variables\n",
    "    y = cp.Parameter((ny,))  # integer decision variables\n",
    "\n",
    "    p = cp.Parameter((nx,))  # linear term in the objective\n",
    "    b = cp.Parameter((nx,))  # RHS of the constraint x <= b\n",
    "    a = cp.Parameter((1,))   # RHS of the constraint 1.T*x <= a\n",
    "    s = cp.Variable((nx,), nonneg=True)  # slack variables\n",
    "    \n",
    "    # Define the QP problem\n",
    "    if penalty == \"l1\": # default to l1 penalty\n",
    "        objective = cp.Minimize(cp.quad_form(x, np.eye(nx)) + p.T @ x + rho1*cp.sum(s))\n",
    "    elif penalty == \"l2\": \n",
    "        objective = cp.Minimize(cp.quad_form(x, np.eye(nx)) + p.T @ x + rho1*cp.quad_form(s, np.eye(nx)))\n",
    "    constraints = [\n",
    "        x <= b,\n",
    "        sum(x) <= a,\n",
    "        x <= bigM * y + s,\n",
    "        s >= 0,\n",
    "    ]\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    # Create CVXPY layer\n",
    "    cvxpylayer = CvxpyLayer(problem, parameters=[p, b, a, y], variables=[x, s])\n",
    "    return cvxpylayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48cde925",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _check_shapes(p, b, a, y):\n",
    "    \"\"\"    Quick assertion helper. Accepts batched tensors:\n",
    "    - p: (B,n), b: (B,n), a: (B,1) or (B,), M: (B,n,ny), y: (B,ny)\n",
    "    \"\"\"\n",
    "    B = p.shape[0]\n",
    "    n = p.shape[1]\n",
    "    ny = y.shape[1]\n",
    "    assert b.shape == (B, n)\n",
    "    assert y.shape == (B, ny)\n",
    "    assert a.shape in [(B,), (B, 1)]\n",
    "    return B, n, ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f3f0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_qp_with_slacks(layer: CvxpyLayer, p, b, a, y):\n",
    "    \"\"\"\n",
    "    Run the CVXPYLayer in batch. All inputs are torch tensors.\n",
    "    Returns x, s (each torch tensor with grad).\n",
    "    \"\"\"\n",
    "    _, _, _ = _check_shapes(p, b, a, y)\n",
    "    if a.ndim == 1:\n",
    "        a = a.unsqueeze(-1)\n",
    "\n",
    "    # Device management\n",
    "    device = p.device\n",
    "    layer = layer.to(device)\n",
    "    p = p.to(device)\n",
    "    b = b.to(device)\n",
    "    a = a.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    x_opt, s_opt = layer(p, b, a, y)\n",
    "    return x_opt, s_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f29f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _solve_single_miqp(args):\n",
    "    \"\"\"Helper function to solve a single MIQP problem.\"\"\"\n",
    "    # Redirect stdout and stderr to devnull at the start of each process\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    sys.stderr = open(os.devnull, 'w')\n",
    "    \"\"\"Helper function to solve a single MIQP problem.\"\"\"\n",
    "    i, p_i, b_i, a_i, nx, ny = args\n",
    "    \n",
    "    # Variables\n",
    "    x = cp.Variable(nx)\n",
    "    y = cp.Variable(ny, boolean=True)\n",
    "    \n",
    "    # Objective and constraints\n",
    "    objective = cp.Minimize(cp.sum_squares(x) + p_i @ x)\n",
    "    constraints = [\n",
    "        x <= b_i,\n",
    "        cp.sum(x) <= a_i,\n",
    "        cp.sum(y) <= 1,\n",
    "        x <= 1e3 * y\n",
    "    ]\n",
    "    \n",
    "    # Problem definition\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    \n",
    "    try:\n",
    "        prob.solve(solver=cp.GUROBI, verbose=False, OutputFlag=0)\n",
    "        \n",
    "        if x.value is not None and y.value is not None:\n",
    "            return i, x.value, y.value\n",
    "        else:\n",
    "            return i, np.zeros(nx), np.zeros(ny)\n",
    "    except Exception as e:\n",
    "        print(f\"Error solving sample {i}: {e}\")\n",
    "        return i, np.zeros(nx), np.zeros(ny)\n",
    "\n",
    "@torch.no_grad()\n",
    "def GUROBI_solve_parallel(p: torch.Tensor, b: torch.Tensor, a: torch.Tensor, max_workers=None):\n",
    "    \"\"\"\n",
    "    Solve MIQP for each sample in the batch using parallel processing.\n",
    "    \"\"\"\n",
    "    device = p.device\n",
    "    p_np = p.detach().cpu().numpy()\n",
    "    b_np = b.detach().cpu().numpy()\n",
    "    a_np = a.detach().cpu().numpy()\n",
    "    \n",
    "    if a_np.ndim == 2:\n",
    "        a_np = a_np.squeeze(-1)\n",
    "    \n",
    "    B = p_np.shape[0]\n",
    "    \n",
    "    # Prepare arguments for parallel execution\n",
    "    args_list = [(i, p_np[i], b_np[i], a_np[i], nx, ny) for i in range(B)]\n",
    "    \n",
    "    # Preallocate result arrays\n",
    "    x_results = np.zeros((B, nx))\n",
    "    y_results = np.zeros((B, ny))\n",
    "    \n",
    "    # Use ProcessPoolExecutor for parallel solving\n",
    "    if max_workers is None:\n",
    "        max_workers = min(B, mp.cpu_count())\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(_solve_single_miqp, args): args[0] for args in args_list}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            i, x_sol, y_sol = future.result()\n",
    "            x_results[i] = x_sol\n",
    "            y_results[i] = y_sol\n",
    "    \n",
    "    return torch.tensor(x_results).float().to(device), torch.tensor(y_results).int().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e847e59",
   "metadata": {},
   "source": [
    "### Batch Solve Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a994eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "p_batch = batch['p']\n",
    "b_batch = batch['b']\n",
    "a_batch = batch['a']\n",
    "B = p_batch.shape[0]\n",
    "M_batch = torch.ones(B, nx, ny) * 50.0  # Big-M matrix\n",
    "\n",
    "device = p_batch.device  # use your current device\n",
    "B, n = p_batch.shape\n",
    "ny = M_batch.shape[-1]\n",
    "\n",
    "# 4.1 Build the layer once\n",
    "cvx_layer = QP_Layer(nx=n, ny=ny, penalty=\"l1\", rho1=1e1)\n",
    "\n",
    "nn_model = MLPWithSTE(insize=2*nx+1, outsize=ny,\n",
    "            bias=True,\n",
    "            linear_map=torch.nn.Linear,\n",
    "            nonlin=nn.ReLU,\n",
    "            hsizes=[128] * 4)\n",
    "\n",
    "# 4.2 Get y from your classifier\n",
    "# Concatenate inputs\n",
    "theta = torch.cat([p_batch, b_batch, a_batch], dim=1)  # shape: (B, nx + ny + 1)\n",
    "# Forward pass through classifier\n",
    "y_pred_hard = nn_model(theta).float()  # shape: (B, ny)\n",
    "y_pred_int = y_pred_hard.int()\n",
    "\n",
    "# 4.3 Solve the convex subproblem given y\n",
    "x_opt, s_opt = solve_qp_with_slacks(\n",
    "    cvx_layer, p_batch, b_batch, a_batch, y_pred_hard\n",
    ")\n",
    "obj_val = torch.sum(x_opt**2, dim=1) + torch.sum(p_batch * x_opt, dim=1)  # (B,)\n",
    "\n",
    "# 4.4 Solve with GUROBI for comparison\n",
    "x_solver, y_solver = GUROBI_solve_parallel(p_batch, b_batch, a_batch)\n",
    "\n",
    "if True:\n",
    "    # print(\"NN Solution x:\", x_opt)\n",
    "    # print(\"GUROBI Solution x:\", x_solver)\n",
    "    print(\"Objective values:\", obj_val)\n",
    "    print(\"Slack:\", s_opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38c9c3",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Self-Supervised Learning Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77931e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some penalty functions that we may use\n",
    "l1_penalty = lambda s: s.sum(dim=1)\n",
    "l2_penalty = lambda s: (s**2).sum(dim=1)\n",
    "\n",
    "def train(nn_model, cvx_layer, train_loader, slack_penalty = l1_penalty, constraint_penalty = torch.relu,\n",
    "            slack_weight = 1e1, constraint_weight = 1e1,\n",
    "            epochs=5, log_every=50, learning_rate=1e-4, device = None):\n",
    "    \n",
    "    # Put all layers in device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    nn_model.to(device)\n",
    "    nn_model.train()\n",
    "    cvx_layer.to(device)\n",
    "\n",
    "    global_step = 0\n",
    "    loss_history = []\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)\n",
    "    supervised_loss_fn = nn.BCEWithLogitsLoss() \n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        for batch in train_loader:\n",
    "            p_batch = batch['p'].to(device)\n",
    "            b_batch = batch['b'].to(device)\n",
    "            a_batch = batch['a'].to(device)\n",
    "            B = p_batch.shape[0]\n",
    "            # ---- Predict y from theta = [p,b,a] ----\n",
    "            theta = torch.cat([p_batch, b_batch, a_batch], dim=-1)  # (B, 2n+1)\n",
    "            y_pred_hard = nn_model(theta).float() # (B, ny), hard {0,1}\n",
    "            # ---- Solve convex subproblem given y ----\n",
    "            # CVXPYLayer supports autograd; keep inputs requiring grad if needed\n",
    "            # x_opt, s_opt = solve_qp_with_slacks(cvx_layer, p_batch, b_batch, a_batch, y_pred_hard)\n",
    "            \n",
    "            # May need it to include a supervised loss function \n",
    "            x_solver, y_solver = GUROBI_solve_parallel(p_batch, b_batch, a_batch)\n",
    "            supervised_loss = supervised_loss_fn(y_pred_hard, y_solver.float())\n",
    "\n",
    "            # obj_val = (x_opt**2).sum(dim=1) + (p_batch * x_opt).sum(dim=1)  # (B,)\n",
    "            # # Slack penalty, for constraint violation of the continuous decision variables\n",
    "            # slack_pen = slack_penalty(s_opt) \n",
    "            # # Violation penalty for constraint violation with the integer decision variables\n",
    "            # y_sum_penalty = constraint_penalty(y_pred_hard.sum(dim=1) - 1.0)  # (B,)\n",
    "            # # Total loss with balanced weights\n",
    "            # weights = torch.tensor([1.0, slack_weight, constraint_weight, 0.0], device=device)\n",
    "            # weights = weights / weights.sum()\n",
    "            # loss = (\n",
    "            #     weights[0]*obj_val.mean() # Main objective (no exp!)\n",
    "            #     + weights[1]*slack_pen.mean() # Heavy slack penalty\n",
    "            #     + weights[2]*y_sum_penalty.mean() # Integer constraint penalty\n",
    "            #     # + weights[3]*y_gt_penalty.mean() # Binary sharpness regularizer\n",
    "            # )\n",
    "\n",
    "            loss = supervised_loss\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            # ---- Backprop ----\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(nn_model.parameters(), max_norm=1e1)\n",
    "            optimizer.step()\n",
    "\n",
    "            # ---- Logging ----\n",
    "            if (global_step % log_every) == 0:\n",
    "                # print(f\"[epoch {epoch} | step {global_step}] \"\n",
    "                #     f\"loss={loss.item():.4f}  obj={obj_val.mean().item():.4f}  \"\n",
    "                #     f\"slack={slack_pen.mean().item():.4f}  sum(y)={y_sum_penalty.mean().item():.3f}\")\n",
    "                print(f\"[epoch {epoch} | step {global_step}] \"\n",
    "                    f\"loss={loss.item():.4f}\")\n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa267c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nn_model = MLPWithSTE(insize=2*nx+1, outsize=ny,\n",
    "                bias=True,\n",
    "                linear_map=torch.nn.Linear,\n",
    "                nonlin=nn.ReLU,\n",
    "                hsizes=[128] * 4)\n",
    "slack_weight = 1e1\n",
    "constraint_weight = 1e3\n",
    "cvx_layer = QP_Layer(nx=nx, ny=ny, penalty=\"l1\", rho1=slack_weight)\n",
    "\n",
    "train(nn_model, cvx_layer, train_loader, \n",
    "    slack_weight = slack_weight, constraint_weight = constraint_weight,\n",
    "    epochs=50, log_every=50, learning_rate=1e-3, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdefb2e3",
   "metadata": {},
   "source": [
    "### Supervised learning demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bfcc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nn_model, train_loader, test_loader, training_params, verbose=True, device = None):\n",
    "    TRAINING_EPOCHS = training_params['TRAINING_EPOCHS']\n",
    "    CHECKPOINT_AFTER = training_params['CHECKPOINT_AFTER']\n",
    "    LEARNING_RATE = training_params['LEARNING_RATE']\n",
    "    WEIGHT_DECAY = training_params['WEIGHT_DECAY']\n",
    "    PATIENCE = training_params['PATIENCE']\n",
    "    # Put all layers in device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    nn_model.to(device)\n",
    "\n",
    "    global_step = 0\n",
    "    loss_history = []\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    supervised_loss_fn = nn.HuberLoss() \n",
    "    best_val_loss = float(\"inf\") # Store best validation \n",
    "    epochs_no_improve = 0  # Count epochs with no improvement       \n",
    "\n",
    "    for epoch in range(1, TRAINING_EPOCHS+1):\n",
    "        nn_model.train()\n",
    "        running_loss = 0.0        \n",
    "        for batch in train_loader:\n",
    "            p_batch = batch['p'].to(device)\n",
    "            b_batch = batch['b'].to(device)\n",
    "            a_batch = batch['a'].to(device)\n",
    "            B = p_batch.shape[0]\n",
    "            # ---- Predict y from theta = [p,b,a] ----\n",
    "            theta = torch.cat([p_batch, b_batch, a_batch], dim=-1)  # (B, 2n+1)\n",
    "            y_pred = nn_model(theta).float() # (B, ny), hard {0,1}\n",
    "            \n",
    "            # May need it to include a supervised loss function \n",
    "            x_solver, y_solver = GUROBI_solve_parallel(p_batch, b_batch, a_batch)\n",
    "\n",
    "            supervised_loss = supervised_loss_fn(y_pred, y_solver.float())\n",
    "\n",
    "            loss = supervised_loss\n",
    "\n",
    "            # ---- Backprop ----\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(nn_model.parameters(), max_norm=1e1)\n",
    "            optimizer.step()\n",
    "            loss_history.append(loss.item())\n",
    "        \n",
    "            # ---- Logging ----\n",
    "            if (global_step % CHECKPOINT_AFTER) == 0:\n",
    "                training_loss = loss.item()\n",
    "                val_loss_total = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for val_batch in test_loader:\n",
    "                        p_batch = val_batch['p'].to(device)\n",
    "                        b_batch = val_batch['b'].to(device)\n",
    "                        a_batch = val_batch['a'].to(device)\n",
    "                        # ---- Predict y from theta = [p,b,a] ----\n",
    "                        theta = torch.cat([p_batch, b_batch, a_batch], dim=-1)  # (B, 2n+1)\n",
    "                        y_pred_test = nn_model(theta).float() # (B, ny), hard {0,1}\n",
    "                        _, y_solver_test = GUROBI_solve_parallel(p_batch, b_batch, a_batch)\n",
    "                        val_loss_total += supervised_loss_fn(y_pred_test, y_solver_test).item()                           \n",
    "                    avg_val_loss = val_loss_total / len(test_loader)\n",
    "\n",
    "                print(f\"[epoch {epoch} | step {global_step}] \"\n",
    "                    f\"training loss = {training_loss:.4f}, \"\n",
    "                    f\"validation loss = {avg_val_loss:.4f}\")\n",
    "                \n",
    "                # Check if need to update the learning rates\n",
    "                last_lr = optimizer.param_groups[0]['lr']\n",
    "                scheduler.step(loss.item())\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                if current_lr != last_lr:\n",
    "                    print(f\"Learning rate updated: {last_lr:.6f} -> {current_lr:.6f}\")\n",
    "                    last_lr = current_lr\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve >= PATIENCE:\n",
    "                        print(\"Early stopping triggered!\")\n",
    "                        break\n",
    "                    \n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778c812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1 | step 0] training loss = 0.3281, validation loss = 0.2530\n",
      "[epoch 1 | step 10] training loss = 0.1328, validation loss = 0.1664\n",
      "[epoch 1 | step 20] training loss = 0.1562, validation loss = 0.1213\n",
      "[epoch 1 | step 30] training loss = 0.0781, validation loss = 0.0816\n",
      "[epoch 1 | step 40] training loss = 0.0703, validation loss = 0.0589\n",
      "[epoch 1 | step 50] training loss = 0.0547, validation loss = 0.0528\n",
      "[epoch 1 | step 60] training loss = 0.0469, validation loss = 0.0553\n",
      "[epoch 1 | step 70] training loss = 0.0469, validation loss = 0.0462\n",
      "[epoch 1 | step 80] training loss = 0.0312, validation loss = 0.0428\n",
      "[epoch 1 | step 90] training loss = 0.0547, validation loss = 0.0405\n",
      "[epoch 1 | step 100] training loss = 0.0156, validation loss = 0.0398\n",
      "[epoch 1 | step 110] training loss = 0.0312, validation loss = 0.0396\n",
      "[epoch 1 | step 120] training loss = 0.0312, validation loss = 0.0315\n",
      "[epoch 1 | step 130] training loss = 0.0156, validation loss = 0.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x743bb883cfe0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu22/.pyenv/versions/3.12.11/lib/python3.12/logging/__init__.py\", line 243, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1 | step 140] training loss = 0.0078, validation loss = 0.0306\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nn_model = MLPWithSTE(insize=2*nx+1, outsize=ny,\n",
    "                bias=True,\n",
    "                linear_map=torch.nn.Linear,\n",
    "                nonlin=nn.ReLU,\n",
    "                hsizes=[128] * 2)\n",
    "\n",
    "training_params = {}\n",
    "training_params['TRAINING_EPOCHS'] = int(50)\n",
    "training_params['CHECKPOINT_AFTER'] = int(10)\n",
    "training_params['LEARNING_RATE'] = 1e-3\n",
    "training_params['WEIGHT_DECAY'] = 1e-5\n",
    "training_params['PATIENCE'] = 10\n",
    "\n",
    "train(nn_model, train_loader, test_loader, training_params, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa707b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b992d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penn_env312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
